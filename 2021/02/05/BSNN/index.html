<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>BSNN - Garlicisnotmyfavor</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Garlicisnotmyfavor"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Garlicisnotmyfavor"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="一篇关于网络协议分类论文的复现工作"><meta property="og:type" content="blog"><meta property="og:title" content="BSNN"><meta property="og:url" content="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/"><meta property="og:site_name" content="Garlicisnotmyfavor"><meta property="og:description" content="一篇关于网络协议分类论文的复现工作"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic3.mac69.com/ico/202010/12105133_55b5814fec.jpeg"><meta property="article:published_time" content="2021-02-05T15:39:55.000Z"><meta property="article:modified_time" content="2021-02-07T14:43:12.000Z"><meta property="article:author" content="Garlic"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://pic3.mac69.com/ico/202010/12105133_55b5814fec.jpeg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/"},"headline":"Garlicisnotmyfavor","image":[],"datePublished":"2021-02-05T15:39:55.000Z","dateModified":"2021-02-07T14:43:12.000Z","author":{"@type":"Person","name":"Garlic"},"description":"一篇关于网络协议分类论文的复现工作"}</script><link rel="canonical" href="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">garlicisnotmyfavor</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://pic3.mac69.com/ico/202010/12105133_55b5814fec.jpeg" alt="BSNN"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-02-05T15:39:55.000Z" title="2021-02-05T15:39:55.000Z">2021-02-05</time>发表</span><span class="level-item"><time dateTime="2021-02-07T14:43:12.000Z" title="2021-02-07T14:43:12.000Z">2021-02-07</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87/">论文</a><span> / </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%88%86%E7%B1%BB/">网络协议分类</a></span><span class="level-item">36 分钟读完 (大约5425个字)</span></div></div><h1 class="title is-3 is-size-4-mobile">BSNN</h1><div class="content"><p>一篇关于网络协议分类论文的复现工作</p>
<a id="more"></a>

<h2 id="论文理解"><a href="#论文理解" class="headerlink" title="论文理解"></a>论文理解</h2><p>本质其实是一个文本分类问题，解决的问题是对网络协议分类，但不同于以往的协议分类问题，这篇论文不使用报文头等特征信息，而是纯粹地从payload特征提取角度对网络协议分类。因此可以看作一个文本分类问题，输入一个packet的payload部分，输出报文协议类型。其优点在于：</p>
<ul>
<li>可以学习新的协议，而不被限制于已有协议</li>
<li>不需要繁复的对报文头特征信息的挖掘（并且这些信息可能是不靠谱的，可以修改的话）</li>
</ul>
<img src="1.png" width = "450" height = "250" align=center />

<p>模型结构如上所示，是一个Hierarchical attention network。</p>
<p>先解释一下模型中的各个字段的意思。Datagram指的是一个packet中的payload部分，如下图wireshark抓包后蓝色的byte部分。进一步对Datagram做划分可以得到很多个长度均等的Segment，这里如果设定每个segment长度都为5的话，一个segment就可以表达为下图中红色方框部分。可以看到一个segment[3b, de, 01, 00, 00]里有多个十六进制字符。</p>
<p>这里为了更好的理解，可以做一个类比：</p>
<ul>
<li>一篇文章 ==&gt;  datagram ==&gt; [3b, de, 01, 00, 00, 01, 00, 00……]</li>
<li>文章中的句子 ==&gt; segment ==&gt; [3b, de, 01, 00, 00]</li>
<li>句子中的单词 ==&gt; 3b</li>
</ul>
<img src="2.png" width = "450" height = "250" align=center />

<p>现在要对这样一个结构的datagram做训练，最后需要学到它所属的协议类型。模型的直觉就是我们首先去关注一个句子（segment）中单词（character）的表达，使用rnn(LSTM/GRU)训练并且每个单词对于这个句子的重要性是不同的（attention机制），通过这样一个过程得到这个句子（segment）的表达。之后再重复类似的过程，在一篇文章（datagram）中，不同句子的重要性不同，关注于“焦点”，通过一个attention encoder得到这篇文章（datagram）的表达，使用softmax等最后得到这个文章（datagram）的类别。</p>
<p>整个模型有两层attention encoder，使用的RNN Unit是LSTM/GRU（我后面实现的GRU，因为对我的电脑友好一些🐶），且是bidirectional的（双向），因为这里上下文都是有意义的，需要全局的。实现细节上使用Focal Loss，主要是为了处理样本数据不均衡的问题（好巧不巧我做数据的时候正好每个类别都是均分的，似乎对我这个没有多大用处，我的错），评估指标使用$F_1$。</p>
<h2 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h2><p>代码目录中data文件夹内为数据，try文件夹中为第一次尝试的pytorch代码（nn结构有问题，当时不太搞明白多层attention怎么写，但也是个实践过程先保存下来），run_BSNN保存的运行数据，pre.py做数据预处理，DataUtil.py做数据padding及划分，BSNN.py放模型，train.py为运行入口。</p>
<p>tensorflow版本为1.14（主要想使用一个老版本的库，降到这个版本，warning还是有的😅），pre使用的pytorch（但是相关数据预处理已运行出来保存好了，因此不需要再使用pytorch运行pre.py）</p>
<h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>论文中数据集是自己收集了，并且在网上找了一下现存数据集，比较难得到条件适合的，因此自己流量抓包收集了300笔数据（多了我电脑伤不起欸😭）。数据特征如下：</p>
<ul>
<li>抓的是DNS，OICQ，QUIC三个协议，每个抓取了100笔，其中后两个是google使用的快传协议和QQ使用的协议</li>
<li>一些报文内容较短，一些很长，这个特点使得做数据padding时较难选择一个合适的统一长度（我这里最后选择的100作为固定长）</li>
<li>因为是连续地抓取，前后报文相似度很多，因此每类协议多样性不足</li>
</ul>
<p>预处理流程如下图所示：</p>
<img src="3.png" width = "550" height = "250" align=center />

<p>pre.py中数据处理代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 一个segment的长度，论文中推荐的是L=8</span></span><br><span class="line">SEGMENT_LEN = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">packets = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面三个都是对报文数据做处理，提取出需要的payload部分，存储下来</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/QUIC.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    content = json.load(reader)</span><br><span class="line">    <span class="comment">#len(content) 先弄小一点的数据集</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">        packet = [<span class="string">&#x27;0&#x27;</span>, content[i][<span class="string">&quot;_source&quot;</span>][<span class="string">&quot;layers&quot;</span>][<span class="string">&quot;udp&quot;</span>][<span class="string">&quot;udp.payload&quot;</span>]]</span><br><span class="line">        packets.append(packet)</span><br><span class="line">    reader.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># QQ的协议</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/OICQ.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    content = json.load(reader)</span><br><span class="line">    <span class="comment">#len(content) 先弄小一点的数据集</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">        packet = [<span class="string">&#x27;1&#x27;</span>, content[i][<span class="string">&quot;_source&quot;</span>][<span class="string">&quot;layers&quot;</span>][<span class="string">&quot;udp&quot;</span>][<span class="string">&quot;udp.payload&quot;</span>]]</span><br><span class="line">        packets.append(packet)</span><br><span class="line">    reader.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/DNS.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    content = json.load(reader)</span><br><span class="line">    <span class="comment">#len(content) 先弄小一点的数据集</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">        packet = [<span class="string">&#x27;2&#x27;</span>, content[i][<span class="string">&quot;_source&quot;</span>][<span class="string">&quot;layers&quot;</span>][<span class="string">&quot;udp&quot;</span>][<span class="string">&quot;udp.payload&quot;</span>]]</span><br><span class="line">        packets.append(packet)</span><br><span class="line">    reader.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># train数据写入csv ==&gt; payload.csv</span></span><br><span class="line">csv_fp = <span class="built_in">open</span>(<span class="string">&quot;data/train.csv&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">writer = csv.writer(csv_fp)</span><br><span class="line">writer.writerow([<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;payload&#x27;</span>])</span><br><span class="line">writer.writerows(packets)</span><br><span class="line">csv_fp.close()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">将payload中字符转化为0-255数字</span></span><br><span class="line"><span class="string">将每个datagram的payload按照长度N=SEGMENT_LEN划分</span></span><br><span class="line"><span class="string">return: [[label, [[0,1,2,3,...][]....]]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment_slice</span>():</span></span><br><span class="line">    data = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">    labels = data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    payloads = data[<span class="string">&#x27;payload&#x27;</span>].apply(<span class="keyword">lambda</span> x :x.split(<span class="string">&#x27;:&#x27;</span>))</span><br><span class="line">    Datagrams = [] <span class="comment"># [[label, [0,255,...]],...]</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(data)):</span><br><span class="line">        Datagram = [] <span class="comment"># [label, [0,255,...]]</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(payloads[i])):</span><br><span class="line">            Datagram.append(<span class="built_in">int</span>(payloads[i][j], <span class="number">16</span>))</span><br><span class="line">        Datagrams.append([labels[i], Datagram])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 切片</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(Datagrams)):</span><br><span class="line">        last_num = <span class="built_in">len</span>(Datagrams[i][<span class="number">1</span>]) % SEGMENT_LEN</span><br><span class="line">        Datagrams[i][<span class="number">1</span>] = Datagrams[i][<span class="number">1</span>] + [<span class="number">0</span>]*(SEGMENT_LEN-last_num)</span><br><span class="line">        temp = np.array(Datagrams[i][<span class="number">1</span>])</span><br><span class="line">        temp = temp.reshape(-<span class="number">1</span>, SEGMENT_LEN)</span><br><span class="line">        Datagrams[i][<span class="number">1</span>] = temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Datagrams</span><br></pre></td></tr></table></figure>

<p>DataUtil.py中进一步处理</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pre <span class="keyword">as</span> pre</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">datagram的segment数量不一致，都统一为maxlen大小 空的填充0向量</span></span><br><span class="line"><span class="string">不足：0的填充比较多</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_data_x</span>(<span class="params">data_xs, maxlen=<span class="number">100</span>, PAD=<span class="number">0</span></span>):</span>  </span><br><span class="line">    padded_data_xs = []</span><br><span class="line">    <span class="keyword">for</span> data_x <span class="keyword">in</span> data_xs:</span><br><span class="line">        <span class="comment"># 一个datagram的</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(data_x) &gt;= maxlen:</span><br><span class="line">            padded_data_x = data_x[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padded_data_x = data_x</span><br><span class="line">            zero_len = maxlen-<span class="built_in">len</span>(padded_data_x)</span><br><span class="line">            zero = np.zeros([zero_len,<span class="number">8</span>], <span class="built_in">int</span>)</span><br><span class="line">            padded_data_x = np.insert(padded_data_x, <span class="built_in">len</span>(data_x), values=zero, axis=<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">        padded_data_xs.append(padded_data_x)</span><br><span class="line">         </span><br><span class="line">    <span class="keyword">return</span> padded_data_xs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_dataset</span>():</span></span><br><span class="line">    num_classes = <span class="number">3</span></span><br><span class="line">    datagrams = pre.segment_slice()</span><br><span class="line"></span><br><span class="line">    random.shuffle(datagrams) <span class="comment"># 打乱数据集</span></span><br><span class="line"></span><br><span class="line">    data_x = []</span><br><span class="line">    data_y = []</span><br><span class="line">    <span class="keyword">for</span> datagram <span class="keyword">in</span> datagrams:</span><br><span class="line">        label = datagram[<span class="number">0</span>]</span><br><span class="line">        labels = [<span class="number">0</span>] * num_classes</span><br><span class="line">        labels[label-<span class="number">1</span>] = <span class="number">1</span> <span class="comment"># 转化为one-hot</span></span><br><span class="line">        data_y.append(labels)</span><br><span class="line">        data_x.append(datagram[<span class="number">1</span>])</span><br><span class="line">    data_x = pad_data_x(data_x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 划分训练集，验证集</span></span><br><span class="line">    length = <span class="built_in">len</span>(data_x)</span><br><span class="line">    train_x, dev_x = data_x[:<span class="built_in">int</span>(length*<span class="number">0.9</span>)], data_x[<span class="built_in">int</span>(length*<span class="number">0.9</span>)+<span class="number">1</span> :]</span><br><span class="line">    train_y, dev_y = data_y[:<span class="built_in">int</span>(length*<span class="number">0.9</span>)], data_y[<span class="built_in">int</span>(length*<span class="number">0.9</span>)+<span class="number">1</span> :]</span><br><span class="line">    <span class="keyword">return</span> train_x, train_y, dev_x, dev_y</span><br></pre></td></tr></table></figure>

<h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>模型主要分为四个部分：</p>
<ul>
<li>word encoder （BiGRU layer）</li>
<li>word attention （Attention layer）</li>
<li>sentence encoder （BiGRU layer）</li>
<li>sentence attention （Attention layer）</li>
</ul>
<h4 id="GRU原理"><a href="#GRU原理" class="headerlink" title="GRU原理"></a>GRU原理</h4><img src="4.png" width = "250" height = "220" align=center />

<p>GRU是RNN的一个变种，使用门机制来记录当前序列的状态。在GRU中有两种类型的门（gate）: reset gate和update gate。这两个门一起控制来决定当前状态有多少信息要更新。</p>
<p>reset gate是用于决定多少过去的信息被用于生成候选状态，如果Rt为0，表明忘记之前的所有状态：<br>$$<br>r_t = \sigma(W_rx_r+U_rh_{y-1}+b_r)<br>$$<br>根据reset gate的分数可以计算出候选状态：<br>$$<br>\hat{h_t} = tanh(W_hx_t+r_t\oplus(U_hh_{t-1}+b_h))<br>$$<br>update gate是用来决定由多少过去的信息被保留，以及多少新信息被加进来：<br>$$<br>z_t = \sigma(W_zx_r+U_zh_{t-1}+b_z)<br>$$<br>最后，隐藏层状态的计算公式，有update gate、候选状态和之前的状态共同决定：<br>$$<br>h_t = (1-z_t)\oplus h_{t-1}+z_t \oplus \hat{h_t}<br>$$</p>
<h4 id="Attention原理"><a href="#Attention原理" class="headerlink" title="Attention原理"></a>Attention原理</h4><img src="5.png" width = "450" height = "200" align=center />

<h4 id="1、word-encoder-layer"><a href="#1、word-encoder-layer" class="headerlink" title="1、word encoder layer"></a>1、word encoder layer</h4><p>首先，将每个segment中的character做embedding转换成向量（这里使用one-hot），然后输入到双向GRU网络中，结合上下文的信息，获得该character对应的隐藏状态输出$h_{it}=[\overrightarrow{h_{it}}, \overleftarrow{h_{it}}]$<br>$$<br>x_{it} = W_cw_{it}, t \in [i,T]<br>$$</p>
<p>$$<br>\overrightarrow{h_{it}} = \overrightarrow{GRU}(x_{it})<br>$$</p>
<p>$$<br>\overleftarrow{h_{it}} = \overleftarrow{GRU}(x_{it})<br>$$</p>
<h4 id="2、word-attention-layer"><a href="#2、word-attention-layer" class="headerlink" title="2、word attention layer"></a>2、word attention layer</h4><p>attention机制的目的就是要把一个segment中，对segment表达最重要的character找出来，赋予一个更大的比重。</p>
<p>首先将word encoder那一步的输出得到的$h_{it}$输入到一个单层的感知机中得到结果$u_{it}$作为其隐含表示<br>$$<br>u_{it} = tanh(W_wh_{it}+b_w)<br>$$<br>接着为了衡量character的重要性，定义了一个随机初始化的character层面上下文向量$u_w$，计算其与segment中每个charater的相似度，然后经过一个<code>softmax</code>操作获得了一个归一化的attention权重矩阵$\alpha_{it}$，代表segement i中第t个charater的权重：</p>
<p>$$<br>\alpha_{it} = \frac {exp(u_{it}^Tu_w)}{\sum_t(exp(u_{it}^Tu_w))}<br>$$<br>于是，segment的向量$s_i$就可以看做是segment中character的向量的加权求和。这里的character层面上下文向量是$u_w$随机初始化并且可以在训练的过程中学习得到的。<br>$$<br>s_i = \sum_{t} \alpha_{it}h_{it}<br>$$</p>
<h4 id="3、sentence-encoder"><a href="#3、sentence-encoder" class="headerlink" title="3、sentence encoder"></a>3、sentence encoder</h4><p>通过上述步骤我们得到了每个segment的向量表示，然后可以用相似的方法得到datagram向量$h_{i}=[\overrightarrow{h_{i}}, \overleftarrow{h_{i}}]$：<br>$$<br>\overleftarrow{h_{i}} = \overleftarrow{GRU}(s_{i}), i \in [i,L]<br>$$</p>
<p>$$<br>\overrightarrow{h_{i}} = \overrightarrow{GRU}(s_{i}), i \in [1,L]<br>$$</p>
<h4 id="4、sentence-attention"><a href="#4、sentence-attention" class="headerlink" title="4、sentence attention"></a>4、sentence attention</h4><p>和character级别的attention类似，使用一个segment级别的上下文向量$u_s$,来衡量一个segment在datagram的重要性。<br>$$<br>u_{i} = tanh(W_sh_{i}+b_s)<br>$$</p>
<p>$$<br>\alpha_i = \frac {exp(u_i^Tu_s)}{\sum_t(exp(u_t^Tu_s))}<br>$$</p>
<p>$$<br>d = \sum_{t} \alpha_{i}h_{i}<br>$$</p>
<h4 id="5、softmax"><a href="#5、softmax" class="headerlink" title="5、softmax"></a>5、softmax</h4><p>上面的$d$向量就是我们的到的最后的datagram表示，然后输入一个全连接的<code>softmax</code>层进行分类就ok了。</p>
<p>BSNN.py中包含了最重要的模型代码:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSequenceLength</span>(<span class="params">sequences</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param sequences: 所有的segmetn长度，[a_size,b_size,c_size,,,]</span></span><br><span class="line"><span class="string">    :return:每个segment进行padding前的实际大小</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    abs_sequences = tf.<span class="built_in">abs</span>(sequences)</span><br><span class="line">    <span class="comment"># after padding data, max is 0</span></span><br><span class="line">    abs_max_seq = tf.reduce_max(abs_sequences, reduction_indices=<span class="number">2</span>)</span><br><span class="line">    max_seq_sign = tf.sign(abs_max_seq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sum is the real length</span></span><br><span class="line">    real_len = tf.reduce_sum(max_seq_sign, reduction_indices=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.cast(real_len, tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BSNN</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, max_sentence_num, max_sentence_length, num_classes, vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_size, learning_rate, decay_steps, decay_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, l2_lambda, grad_clip, is_training=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 initializer=tf.random_normal_initializer(<span class="params">stddev=<span class="number">0.1</span></span>)</span>):</span></span><br><span class="line">        </span><br><span class="line">        self.vocab_size = vocab_size <span class="comment"># 对应我的char数量 256</span></span><br><span class="line">        self.max_sentence_num = max_sentence_num </span><br><span class="line">        self.max_sentence_length = max_sentence_length</span><br><span class="line">        self.num_classes = num_classes <span class="comment"># 分类数 3</span></span><br><span class="line">        self.embedding_size = embedding_size <span class="comment"># 对于char的emb_size=256 论文中初始化为one-hot</span></span><br><span class="line">        self.hidden_size = hidden_size </span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.decay_rate = decay_rate</span><br><span class="line">        self.decay_steps = decay_steps</span><br><span class="line">        self.l2_lambda = l2_lambda <span class="comment"># l2范数</span></span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line">        self.initializer = initializer</span><br><span class="line"></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>, name=<span class="string">&#x27;global_step&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># placeholder</span></span><br><span class="line">        self.input_x = tf.placeholder(tf.int32, [<span class="literal">None</span>, max_sentence_num, max_sentence_length], name=<span class="string">&#x27;input_x&#x27;</span>)</span><br><span class="line">        self.input_y = tf.placeholder(tf.int32, [<span class="literal">None</span>, num_classes], name=<span class="string">&#x27;input_y&#x27;</span>)</span><br><span class="line">        self.dropout_keep_prob = tf.placeholder(tf.float32, name=<span class="string">&#x27;dropout_keep_prob&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        word_embedding = self.word2vec() <span class="comment"># 对于char表达，初始化为one-hot</span></span><br><span class="line">        sen_vec = self.sen2vec(word_embedding) <span class="comment"># 对应一个segment表达</span></span><br><span class="line">        doc_vec = self.doc2vec(sen_vec) <span class="comment"># 对应datagram表达</span></span><br><span class="line"></span><br><span class="line">        self.logits = self.inference(doc_vec)</span><br><span class="line">        self.loss_val = self.loss(self.input_y, self.logits)</span><br><span class="line">        self.train_op = self.train()</span><br><span class="line">        self.prediction = tf.argmax(self.logits, axis=<span class="number">1</span>, name=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">        self.pred_min = tf.reduce_min(self.prediction)</span><br><span class="line">        self.pred_max = tf.reduce_max(self.prediction)</span><br><span class="line">        self.pred_cnt = tf.bincount(tf.cast(self.prediction, dtype=tf.int32))</span><br><span class="line">        self.label_cnt = tf.bincount(tf.cast(tf.argmax(self.input_y, axis=<span class="number">1</span>), dtype=tf.int32))</span><br><span class="line">        self.accuracy = self.accuracy(self.logits, self.input_y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2vec</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;embedding&#x27;</span>):</span><br><span class="line">            self.embedding_mat = tf.Variable(tf.eye(self.vocab_size), name=<span class="string">&#x27;embedding&#x27;</span>) <span class="comment"># 构造256维度单位one-hot向量</span></span><br><span class="line">            <span class="comment"># [batch, sen_in_doc, wrd_in_sent, embedding_size]</span></span><br><span class="line">            word_embedding = tf.nn.embedding_lookup(self.embedding_mat, self.input_x)</span><br><span class="line">            <span class="keyword">return</span> word_embedding</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BidirectionalGRUEncoder</span>(<span class="params">self, inputs, name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        双向GRU编码层，将一segment中的所有character或者一个datagram中的所有segment进行编码得到一个2xhidden_size的输出向量</span></span><br><span class="line"><span class="string">        然后在输入inputs的shape是：</span></span><br><span class="line"><span class="string">        input:[batch, max_time, embedding_size]</span></span><br><span class="line"><span class="string">        output:[batch, max_time, 2*hidden_size]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(name), tf.variable_scope(name, reuse=tf.AUTO_REUSE):  </span><br><span class="line">            fw_gru_cell = rnn.GRUCell(self.hidden_size)</span><br><span class="line">            bw_gru_cell = rnn.GRUCell(self.hidden_size)</span><br><span class="line">            fw_gru_cell = rnn.DropoutWrapper(fw_gru_cell, output_keep_prob=self.dropout_keep_prob)</span><br><span class="line">            bw_gru_cell = rnn.DropoutWrapper(bw_gru_cell, output_keep_prob=self.dropout_keep_prob)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># fw_outputs和bw_outputs的size都是[batch_size, max_time, hidden_size]</span></span><br><span class="line">            (fw_outputs, bw_outputs), (fw_outputs_state, bw_outputs_state) = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">                cell_fw=fw_gru_cell, cell_bw=bw_gru_cell, inputs=inputs,</span><br><span class="line">                sequence_length=getSequenceLength(inputs), dtype=tf.float32</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># outputs的shape是[batch_size, max_time, hidden_size*2]</span></span><br><span class="line">            outputs = tf.concat((fw_outputs, bw_outputs), <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">AttentionLayer</span>(<span class="params">self, inputs, name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inputs是GRU层的输出</span></span><br><span class="line"><span class="string">        inputs: [batch, max_time, 2*hidden_size]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(name):</span><br><span class="line">            context_weight = tf.Variable(tf.truncated_normal([self.hidden_size*<span class="number">2</span>]), name=<span class="string">&#x27;context_weight&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用单层MLP对GRU的输出进行编码，得到隐藏层表示</span></span><br><span class="line">            <span class="comment"># uit =tanh(Wwhit + bw)</span></span><br><span class="line">            fc = layers.fully_connected(inputs, self.hidden_size*<span class="number">2</span>, activation_fn=tf.nn.tanh)</span><br><span class="line"></span><br><span class="line">            multiply = tf.multiply(fc, context_weight)</span><br><span class="line">            reduce_sum = tf.reduce_sum(multiply, axis=<span class="number">2</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># shape: [batch_size, max_time, 1]</span></span><br><span class="line">            alpha = tf.nn.softmax(reduce_sum, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># shape: [batch_size, hidden_size*2]</span></span><br><span class="line">            atten_output = tf.reduce_sum(tf.multiply(inputs, alpha), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> atten_output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sen2vec</span>(<span class="params">self, word_embeded</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;sen2vec&#x27;</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            GRU的输入tensor是[batch_size, max_time,...]，在构造segment向量时max_time应该是每个segment的长度，</span></span><br><span class="line"><span class="string">            所以这里将batch_size*sen_in_doc当做是batch_size，这样一来，每个GRU的cell处理的都是一个character的向量</span></span><br><span class="line"><span class="string">            并最终将一segment中的所有character的向量融合（Attention）在一起形成segment向量</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="comment"># shape：[batch_size * sen_in_doc, word_in_sent, embedding_size]</span></span><br><span class="line">            word_embeded = tf.reshape(word_embeded, [-<span class="number">1</span>, self.max_sentence_length, self.embedding_size])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># shape: [batch_size * sen_in_doc, word_in_sent, hiddeng_size * 2]</span></span><br><span class="line">            word_encoder = self.BidirectionalGRUEncoder(word_embeded, name=<span class="string">&#x27;word_encoder&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># shape: [batch_size * sen_in_doc, hidden_size * 2]</span></span><br><span class="line">            sen_vec = self.AttentionLayer(word_encoder, name=<span class="string">&#x27;word_attention&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> sen_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doc2vec</span>(<span class="params">self, sen_vec</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;doc2vec&#x27;</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            跟sen2vec类似，不过这里每个cell处理的是一个segment的向量，最后融合成为datagram的向量</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            sen_vec = tf.reshape(sen_vec, [-<span class="number">1</span>, self.max_sentence_num, self.hidden_size*<span class="number">2</span>])</span><br><span class="line">            <span class="comment"># shape: [batch_size，sen_in_doc, hidden_size * 2]</span></span><br><span class="line">            doc_encoder = self.BidirectionalGRUEncoder(sen_vec, name=<span class="string">&#x27;doc_encoder&#x27;</span>)</span><br><span class="line">            <span class="comment"># shape: [batch_size，hidden_size * 2]</span></span><br><span class="line">            doc_vec = self.AttentionLayer(doc_encoder, name=<span class="string">&#x27;doc_vec&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> doc_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">self, doc_vec</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;logits&#x27;</span>):</span><br><span class="line">            fc_out = layers.fully_connected(doc_vec, self.num_classes)</span><br><span class="line">            <span class="keyword">return</span> fc_out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">self, logits, input_y</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy&#x27;</span>):</span><br><span class="line">            predict = tf.argmax(logits, axis=<span class="number">1</span>, name=<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">            label = tf.argmax(input_y, axis=<span class="number">1</span>, name=<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">            acc = tf.reduce_mean(tf.cast(tf.equal(predict, label), tf.float32))</span><br><span class="line">            <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, input_y, logits</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">            losses = tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=logits)</span><br><span class="line">            loss = tf.reduce_mean(losses)</span><br><span class="line">            <span class="keyword">if</span> self.l2_lambda &gt;<span class="number">0</span>:</span><br><span class="line">                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) <span class="keyword">for</span> cand_var <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> <span class="string">&#x27;bia&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> cand_var.name])</span><br><span class="line">                loss += self.l2_lambda * l2_loss</span><br><span class="line">            <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,</span><br><span class="line">                                                   self.decay_steps, self.decay_rate, staircase=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># use grad_clip to hand exploding or vanishing gradients</span></span><br><span class="line">        optimizer = tf.train.AdamOptimizer(learning_rate)</span><br><span class="line">        grads_and_vars = optimizer.compute_gradients(self.loss_val)</span><br><span class="line">        <span class="keyword">for</span> idx, (grad, var) <span class="keyword">in</span> <span class="built_in">enumerate</span>(grads_and_vars):</span><br><span class="line">            <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                grads_and_vars[idx] = (tf.clip_by_norm(grad, self.grad_clip), var)</span><br><span class="line"></span><br><span class="line">        train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)</span><br><span class="line">        <span class="keyword">return</span> train_op</span><br></pre></td></tr></table></figure>

<h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>训练参数的设定可以看下面代码中configuration的内容，这里说一下论文中提到的几个参数，$\gamma = 0$，$\alpha$均等，segment长度设定为8，batch_size为30（与原文一致），epoch=10。</p>
<p>train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> BSNN <span class="keyword">import</span> BSNN</span><br><span class="line"><span class="keyword">from</span> DataUtil <span class="keyword">import</span> read_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment">#configuration</span></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;learning_rate&quot;</span>,<span class="number">0.001</span>,<span class="string">&quot;learning rate&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;num_epochs&quot;</span>,<span class="number">10</span>,<span class="string">&quot;embedding size&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;batch_size&quot;</span>, <span class="number">30</span>, <span class="string">&quot;Batch size for training/evaluating.&quot;</span>) <span class="comment">#批处理的大小 32--&gt;128</span></span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;num_classes&quot;</span>, <span class="number">3</span>, <span class="string">&quot;number of classes&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;vocab_size&quot;</span>, <span class="number">256</span>, <span class="string">&quot;vocabulary size&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;decay_steps&quot;</span>, <span class="number">12000</span>, <span class="string">&quot;how many steps before decay learning rate.&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;decay_rate&quot;</span>, <span class="number">0.9</span>, <span class="string">&quot;Rate of decay for learning rate.&quot;</span>) <span class="comment">#0.5一次衰减多少</span></span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(<span class="string">&quot;ckpt_dir&quot;</span>,<span class="string">&quot;text_han_checkpoint/&quot;</span>,<span class="string">&quot;checkpoint location for the model&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;num_checkpoints&#x27;</span>,<span class="number">5</span>,<span class="string">&#x27;save checkpoints count&#x27;</span>)  </span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;max_sentence_num&#x27;</span>,<span class="number">100</span>,<span class="string">&#x27;max sentence num in a doc&#x27;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;max_sentence_length&#x27;</span>,<span class="number">8</span>,<span class="string">&#x27;max word count in a sentence&#x27;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;embedding_size&quot;</span>,<span class="number">256</span>,<span class="string">&quot;embedding size&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;hidden_size&#x27;</span>,<span class="number">50</span>,<span class="string">&#x27;cell output size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_boolean(<span class="string">&quot;is_training&quot;</span>,<span class="literal">True</span>,<span class="string">&quot;is traning.true:tranining,false:testing/inference&quot;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;validate_every&quot;</span>, <span class="number">100</span>, <span class="string">&quot;Validate every validate_every epochs.&quot;</span>) <span class="comment">#每10轮做一次验证</span></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&#x27;validation_percentage&#x27;</span>,<span class="number">0.1</span>,<span class="string">&#x27;validat data percentage in train data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;dropout_keep_prob&quot;</span>, <span class="number">0.5</span>, <span class="string">&quot;Dropout keep probability (default: 0.5)&quot;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;l2_reg_lambda&quot;</span>, <span class="number">0.0001</span>, <span class="string">&quot;L2 regularization lambda (default: 0.0)&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&#x27;grad_clip&#x27;</span>,<span class="number">2.0</span>,<span class="string">&#x27;grad_clip&#x27;</span>) <span class="comment"># 和类别数相关</span></span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_boolean(<span class="string">&quot;allow_soft_placement&quot;</span>, <span class="literal">True</span>, <span class="string">&quot;Allow device soft device placement&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_boolean(<span class="string">&quot;log_device_placement&quot;</span>, <span class="literal">False</span>, <span class="string">&quot;Log placement of ops on devices&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FLAGS = tf.flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="comment"># load dataset</span></span><br><span class="line">train_x, train_y, dev_x, dev_y = read_dataset()</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;data load finished&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    sess_conf = tf.ConfigProto(</span><br><span class="line">        allow_soft_placement=FLAGS.allow_soft_placement,</span><br><span class="line">        log_device_placement=FLAGS.log_device_placement)</span><br><span class="line">    sess = tf.Session(config=sess_conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> sess.as_default():</span><br><span class="line">        timestamp = <span class="built_in">str</span>(<span class="built_in">int</span>(time.time()))</span><br><span class="line">        out_dir = os.path.abspath(os.path.join(os.path.curdir, <span class="string">&#x27;run_BSNN&#x27;</span>, timestamp))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_dir):</span><br><span class="line">            os.makedirs(out_dir)</span><br><span class="line"></span><br><span class="line">        checkpoint_dir = os.path.abspath(os.path.join(out_dir, FLAGS.ckpt_dir))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</span><br><span class="line">            os.makedirs(checkpoint_dir)</span><br><span class="line"></span><br><span class="line">        checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">&#x27;model&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        bsnn = BSNN(FLAGS.max_sentence_num, FLAGS.max_sentence_length, FLAGS.num_classes, FLAGS.vocab_size, FLAGS.embedding_size,</span><br><span class="line">                  FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.hidden_size, FLAGS.l2_reg_lambda,</span><br><span class="line">                  FLAGS.grad_clip, FLAGS.is_training)</span><br><span class="line">        </span><br><span class="line">        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">x_batch, y_batch</span>):</span></span><br><span class="line">            feed_dict = &#123;bsnn.input_x: x_batch,</span><br><span class="line">                         bsnn.input_y: y_batch,</span><br><span class="line">                         bsnn.dropout_keep_prob: FLAGS.dropout_keep_prob</span><br><span class="line">                         &#125;</span><br><span class="line">            tmp, step, loss, accuracy, label_cnt, pred_cnt, pred_min, pred_max = sess.run(</span><br><span class="line">                [bsnn.train_op, bsnn.global_step, bsnn.loss_val, bsnn.accuracy, bsnn.label_cnt, bsnn.pred_cnt,</span><br><span class="line">                 bsnn.pred_min, bsnn.pred_max,], feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">&#x27;train_label_cnt: &#x27;</span>, label_cnt)</span><br><span class="line">            print(<span class="string">&#x27;train_min_max:&#x27;</span>, pred_min, pred_max)</span><br><span class="line">            print(<span class="string">&#x27;train_cnt: &#x27;</span>, pred_cnt)</span><br><span class="line">            time_str = datetime.datetime.now().isoformat()</span><br><span class="line">            print(<span class="string">&quot;&#123;&#125;:step &#123;&#125;, loss &#123;:g&#125;, acc &#123;:g&#125;&quot;</span>.<span class="built_in">format</span>(time_str, step, loss, accuracy))</span><br><span class="line">            <span class="keyword">return</span> step</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dev_step</span>(<span class="params">dev_x, dev_y</span>):</span></span><br><span class="line">            feed_dict = &#123;bsnn.input_x: dev_x,</span><br><span class="line">                         bsnn.input_y: dev_y,</span><br><span class="line">                         bsnn.dropout_keep_prob: <span class="number">1.0</span></span><br><span class="line">                         &#125;</span><br><span class="line">            step, loss, accuracy, label_cnt, pred_cnt = sess.run(</span><br><span class="line">                [bsnn.global_step, bsnn.loss_val, bsnn.accuracy, bsnn.label_cnt, bsnn.pred_cnt,], feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">            time_str = datetime.datetime.now().isoformat()</span><br><span class="line">            print(<span class="string">&quot;dev result: &#123;&#125;:step &#123;&#125;, loss &#123;:g&#125;, acc &#123;:g&#125;&quot;</span>.<span class="built_in">format</span>(time_str, step, loss, accuracy))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.num_epochs):</span><br><span class="line">            print(<span class="string">&#x27;current epoch %s&#x27;</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">270</span>, FLAGS.batch_size):</span><br><span class="line">                x = train_x[i:i + FLAGS.batch_size]</span><br><span class="line">                y = train_y[i:i + FLAGS.batch_size]</span><br><span class="line">                train_step(x, y)</span><br><span class="line">                cur_step = tf.train.global_step(sess, bsnn.global_step)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            dev_step(dev_x, dev_y)</span><br><span class="line">            path = saver.save(sess, checkpoint_prefix, global_step=epoch)</span><br><span class="line">            print(<span class="string">&#x27;Saved model checkpoint to &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(path))</span><br></pre></td></tr></table></figure>

<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><table>
<thead>
<tr>
<th>epoch</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>loss 1.17914, acc 0.206897</td>
</tr>
<tr>
<td>2</td>
<td>loss 1.16526, acc 0.206897</td>
</tr>
<tr>
<td>3</td>
<td>loss 1.17013, acc 0.206897</td>
</tr>
<tr>
<td>4</td>
<td>loss 1.16233, acc 0.206897</td>
</tr>
<tr>
<td>5</td>
<td>loss 1.14642, acc 0.448276</td>
</tr>
<tr>
<td>6</td>
<td>loss 1.04893, acc 0.448276</td>
</tr>
<tr>
<td>7</td>
<td>loss 0.898174, acc 0.448276</td>
</tr>
<tr>
<td>8</td>
<td>loss 0.708184, acc 0.827586</td>
</tr>
<tr>
<td>9</td>
<td>loss 0.616511, acc 0.827586</td>
</tr>
<tr>
<td>10</td>
<td>loss 0.4711, acc 0.827586</td>
</tr>
</tbody></table>
<p>部分输出细节见下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">current epoch 1</span><br><span class="line">2021-02-05T23:28:26.090626:step 1, loss 1.18368, acc 0.233333</span><br><span class="line">2021-02-05T23:28:26.623201:step 2, loss 1.17141, acc 0.366667</span><br><span class="line">2021-02-05T23:28:27.112586:step 3, loss 1.17092, acc 0.2</span><br><span class="line">2021-02-05T23:28:27.600048:step 4, loss 1.16999, acc 0.333333</span><br><span class="line">2021-02-05T23:28:28.092669:step 5, loss 1.16888, acc 0.4</span><br><span class="line">2021-02-05T23:28:28.593807:step 6, loss 1.1664, acc 0.4</span><br><span class="line">2021-02-05T23:28:29.085813:step 7, loss 1.17391, acc 0.233333</span><br><span class="line">2021-02-05T23:28:29.577284:step 8, loss 1.16082, acc 0.433333</span><br><span class="line">2021-02-05T23:28:30.078963:step 9, loss 1.17725, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:30.484341:step 9, loss 1.17914, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-0</span><br><span class="line"></span><br><span class="line">current epoch 2</span><br><span class="line">2021-02-05T23:28:31.313493:step 10, loss 1.14458, acc 0.466667</span><br><span class="line">2021-02-05T23:28:31.801910:step 11, loss 1.16036, acc 0.366667</span><br><span class="line">2021-02-05T23:28:32.340277:step 12, loss 1.20258, acc 0.2</span><br><span class="line">2021-02-05T23:28:32.840851:step 13, loss 1.16389, acc 0.333333</span><br><span class="line">2021-02-05T23:28:33.345806:step 14, loss 1.14916, acc 0.4</span><br><span class="line">2021-02-05T23:28:33.865011:step 15, loss 1.15131, acc 0.4</span><br><span class="line">2021-02-05T23:28:34.375607:step 16, loss 1.16807, acc 0.233333</span><br><span class="line">2021-02-05T23:28:34.888499:step 17, loss 1.15018, acc 0.433333</span><br><span class="line">2021-02-05T23:28:35.416810:step 18, loss 1.16337, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:35.609295:step 18, loss 1.16526, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-1</span><br><span class="line"></span><br><span class="line">current epoch 3</span><br><span class="line">2021-02-05T23:28:36.307796:step 19, loss 1.14345, acc 0.466667</span><br><span class="line">2021-02-05T23:28:36.807924:step 20, loss 1.15106, acc 0.366667</span><br><span class="line">2021-02-05T23:28:37.413876:step 21, loss 1.18131, acc 0.2</span><br><span class="line">2021-02-05T23:28:37.979364:step 22, loss 1.1481, acc 0.333333</span><br><span class="line">2021-02-05T23:28:38.528895:step 23, loss 1.13485, acc 0.4</span><br><span class="line">2021-02-05T23:28:39.053765:step 24, loss 1.14369, acc 0.4</span><br><span class="line">2021-02-05T23:28:39.566839:step 25, loss 1.16878, acc 0.233333</span><br><span class="line">2021-02-05T23:28:40.087222:step 26, loss 1.1341, acc 0.433333</span><br><span class="line">2021-02-05T23:28:40.603841:step 27, loss 1.1617, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:40.796326:step 27, loss 1.17013, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-2</span><br><span class="line"></span><br><span class="line">current epoch 4</span><br><span class="line">2021-02-05T23:28:41.602442:step 28, loss 1.1145, acc 0.466667</span><br><span class="line">2021-02-05T23:28:42.144761:step 29, loss 1.14097, acc 0.366667</span><br><span class="line">2021-02-05T23:28:42.693294:step 30, loss 1.20376, acc 0.2</span><br><span class="line">2021-02-05T23:28:43.229859:step 31, loss 1.14151, acc 0.333333</span><br><span class="line">2021-02-05T23:28:43.755452:step 32, loss 1.12037, acc 0.4</span><br><span class="line">2021-02-05T23:28:44.263125:step 33, loss 1.13507, acc 0.4</span><br><span class="line">2021-02-05T23:28:44.771254:step 34, loss 1.16051, acc 0.233333</span><br><span class="line">2021-02-05T23:28:45.294753:step 35, loss 1.11591, acc 0.433333</span><br><span class="line">2021-02-05T23:28:45.826330:step 36, loss 1.14527, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:46.004852:step 36, loss 1.16233, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-3</span><br><span class="line"></span><br><span class="line">current epoch 5</span><br><span class="line">2021-02-05T23:28:46.711379:step 37, loss 1.10953, acc 0.466667</span><br><span class="line">2021-02-05T23:28:47.210262:step 38, loss 1.13091, acc 0.366667</span><br><span class="line">2021-02-05T23:28:47.773246:step 39, loss 1.18087, acc 0.2</span><br><span class="line">2021-02-05T23:28:48.286123:step 40, loss 1.13255, acc 0.333333</span><br><span class="line">2021-02-05T23:28:48.789008:step 41, loss 1.11187, acc 0.4</span><br><span class="line">2021-02-05T23:28:49.302116:step 42, loss 1.13908, acc 0.4</span><br><span class="line">2021-02-05T23:28:49.805403:step 43, loss 1.1431, acc 0.266667</span><br><span class="line">2021-02-05T23:28:50.314138:step 44, loss 1.11676, acc 0.433333</span><br><span class="line">2021-02-05T23:28:50.827770:step 45, loss 1.12081, acc 0.366667</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:51.000757:step 45, loss 1.14642, acc 0.448276</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-4</span><br><span class="line"></span><br><span class="line">current epoch 6</span><br><span class="line">2021-02-05T23:28:51.710229:step 46, loss 1.1051, acc 0.566667</span><br><span class="line">2021-02-05T23:28:52.197731:step 47, loss 1.09202, acc 0.5</span><br><span class="line">2021-02-05T23:28:52.747259:step 48, loss 1.15978, acc 0.333333</span><br><span class="line">2021-02-05T23:28:53.266820:step 49, loss 1.09888, acc 0.5</span><br><span class="line">2021-02-05T23:28:53.778483:step 50, loss 1.04828, acc 0.6</span><br><span class="line">2021-02-05T23:28:54.300956:step 51, loss 1.10639, acc 0.5</span><br><span class="line">2021-02-05T23:28:54.807825:step 52, loss 1.06262, acc 0.5</span><br><span class="line">2021-02-05T23:28:55.310532:step 53, loss 1.03075, acc 0.5</span><br><span class="line">2021-02-05T23:28:55.833391:step 54, loss 1.02559, acc 0.466667</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:56.026642:step 54, loss 1.04893, acc 0.448276</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-5</span><br><span class="line"></span><br><span class="line">current epoch 7</span><br><span class="line">2021-02-05T23:28:56.747713:step 55, loss 0.996561, acc 0.566667</span><br><span class="line">2021-02-05T23:28:57.233785:step 56, loss 0.973121, acc 0.5</span><br><span class="line">2021-02-05T23:28:57.788470:step 57, loss 1.10553, acc 0.333333</span><br><span class="line">2021-02-05T23:28:58.301127:step 58, loss 0.873215, acc 0.5</span><br><span class="line">2021-02-05T23:28:58.816781:step 59, loss 0.839209, acc 0.633333</span><br><span class="line">2021-02-05T23:28:59.331319:step 60, loss 0.928375, acc 0.533333</span><br><span class="line">2021-02-05T23:28:59.847088:step 61, loss 0.837925, acc 0.5</span><br><span class="line">2021-02-05T23:29:00.344677:step 62, loss 0.871867, acc 0.533333</span><br><span class="line">2021-02-05T23:29:00.813062:step 63, loss 0.89659, acc 0.5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:00.985665:step 63, loss 0.898174, acc 0.448276</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-6</span><br><span class="line"></span><br><span class="line">current epoch 8</span><br><span class="line">2021-02-05T23:29:01.699028:step 64, loss 0.867806, acc 0.633333</span><br><span class="line">2021-02-05T23:29:02.186859:step 65, loss 0.826654, acc 0.633333</span><br><span class="line">2021-02-05T23:29:02.737684:step 66, loss 1.01797, acc 0.433333</span><br><span class="line">2021-02-05T23:29:03.243936:step 67, loss 0.765, acc 0.666667</span><br><span class="line">2021-02-05T23:29:03.746920:step 68, loss 0.721655, acc 0.8</span><br><span class="line">2021-02-05T23:29:04.245246:step 69, loss 0.937242, acc 0.8</span><br><span class="line">2021-02-05T23:29:04.742633:step 70, loss 0.662761, acc 0.766667</span><br><span class="line">2021-02-05T23:29:05.246319:step 71, loss 0.838433, acc 0.8</span><br><span class="line">2021-02-05T23:29:05.758371:step 72, loss 0.69352, acc 0.833333</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:05.941858:step 72, loss 0.708184, acc 0.827586</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-7</span><br><span class="line"></span><br><span class="line">current epoch 9</span><br><span class="line">2021-02-05T23:29:06.681740:step 73, loss 0.615287, acc 0.833333</span><br><span class="line">2021-02-05T23:29:07.155841:step 74, loss 0.634121, acc 0.833333</span><br><span class="line">2021-02-05T23:29:07.706681:step 75, loss 0.863259, acc 0.7</span><br><span class="line">2021-02-05T23:29:08.233564:step 76, loss 0.527865, acc 0.866667</span><br><span class="line">2021-02-05T23:29:08.728556:step 77, loss 0.626868, acc 0.8</span><br><span class="line">2021-02-05T23:29:09.239092:step 78, loss 0.655192, acc 0.833333</span><br><span class="line">2021-02-05T23:29:09.747197:step 79, loss 0.518165, acc 0.833333</span><br><span class="line">2021-02-05T23:29:10.261518:step 80, loss 0.557925, acc 0.866667</span><br><span class="line">2021-02-05T23:29:10.763176:step 81, loss 0.600361, acc 0.833333</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:10.931726:step 81, loss 0.616511, acc 0.827586</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-8</span><br><span class="line"></span><br><span class="line">current epoch 10</span><br><span class="line">2021-02-05T23:29:11.619203:step 82, loss 0.579097, acc 0.833333</span><br><span class="line">2021-02-05T23:29:12.099981:step 83, loss 0.531345, acc 0.833333</span><br><span class="line">2021-02-05T23:29:12.636098:step 84, loss 0.777414, acc 0.7</span><br><span class="line">2021-02-05T23:29:13.135171:step 85, loss 0.438025, acc 0.866667</span><br><span class="line">2021-02-05T23:29:13.644192:step 86, loss 0.477813, acc 0.833333</span><br><span class="line">2021-02-05T23:29:14.140262:step 87, loss 0.545707, acc 0.833333</span><br><span class="line">2021-02-05T23:29:14.631534:step 88, loss 0.440086, acc 0.833333</span><br><span class="line">2021-02-05T23:29:15.129455:step 89, loss 0.427449, acc 0.866667</span><br><span class="line">2021-02-05T23:29:15.646563:step 90, loss 0.474299, acc 0.833333</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:15.826884:step 90, loss 0.4711, acc 0.827586</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-9</span><br></pre></td></tr></table></figure>

<h3 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h3><ul>
<li>数据集收集时间比较仓促，因此样本不够满意，数据量由于电脑性能限制也比较少</li>
<li>Focal Loss针对我的数据特点使用$\alpha=[[1],[1],[1]],\gamma=0$等同于<code>softmax_cross_entropy_with_logits</code>，效果不错，但没有做详尽的对比实验较难看出它的优势（后一篇文章我已经做了修改）</li>
<li>embedding的维度设置的256，但感觉数据表示比较稀疏</li>
<li>对于character的这一层的训练，我的直觉上来说是远不如单词训练之于句子的意义性，因为相同的byte可能不具有很强的关联性，如果直接将segment的数字表示作为输入，减少一层不知道效果是否会退化很多</li>
</ul>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8624128">https://ieeexplore.ieee.org/abstract/document/8624128</a></p>
<p><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">https://www.aclweb.org/anthology/N16-1174.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zihuaweng.github.io/2018/04/01/loss/">https://zihuaweng.github.io/2018/04/01/loss/</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80594704">https://zhuanlan.zhihu.com/p/80594704</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35571412">https://zhuanlan.zhihu.com/p/35571412</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>BSNN</p><p><a href="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/">http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Garlic</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-02-05</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-02-07</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/02/07/FocalLoss/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">FocalLoss</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/01/21/iceberg/"><span class="level-item">iceberg</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/';
            this.page.identifier = '2021/02/05/BSNN/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'garlicisnotmyfavor' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="Garlic"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Garlic</p><p class="is-size-6 is-block">一生温暖纯良 不舍爱与自由</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">27</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/garlicisnotmyfavor" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/garlicisnotmyfavor"><i class="fab fa-github"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#论文理解"><span class="level-left"><span class="level-item">1</span><span class="level-item">论文理解</span></span></a></li><li><a class="level is-mobile" href="#代码复现"><span class="level-left"><span class="level-item">2</span><span class="level-item">代码复现</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据预处理"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">数据预处理</span></span></a></li><li><a class="level is-mobile" href="#模型构建"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">模型构建</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#GRU原理"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">GRU原理</span></span></a></li><li><a class="level is-mobile" href="#Attention原理"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">Attention原理</span></span></a></li><li><a class="level is-mobile" href="#1、word-encoder-layer"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">1、word encoder layer</span></span></a></li><li><a class="level is-mobile" href="#2、word-attention-layer"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">2、word attention layer</span></span></a></li><li><a class="level is-mobile" href="#3、sentence-encoder"><span class="level-left"><span class="level-item">2.2.5</span><span class="level-item">3、sentence encoder</span></span></a></li><li><a class="level is-mobile" href="#4、sentence-attention"><span class="level-left"><span class="level-item">2.2.6</span><span class="level-item">4、sentence attention</span></span></a></li><li><a class="level is-mobile" href="#5、softmax"><span class="level-left"><span class="level-item">2.2.7</span><span class="level-item">5、softmax</span></span></a></li></ul></li><li><a class="level is-mobile" href="#训练参数"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">训练参数</span></span></a></li><li><a class="level is-mobile" href="#结果"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">结果</span></span></a></li><li><a class="level is-mobile" href="#不足"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">不足</span></span></a></li></ul></li><li><a class="level is-mobile" href="#参考链接"><span class="level-left"><span class="level-item">3</span><span class="level-item">参考链接</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">garlicisnotmyfavor</a><p class="is-size-7"><span>&copy; 2021 Garlic</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>