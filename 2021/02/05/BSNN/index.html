<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>BSNN - Garlicisnotmyfavor</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Garlicisnotmyfavor"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Garlicisnotmyfavor"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="ä¸€ç¯‡å…³äºç½‘ç»œåè®®åˆ†ç±»è®ºæ–‡çš„å¤ç°å·¥ä½œ"><meta property="og:type" content="blog"><meta property="og:title" content="BSNN"><meta property="og:url" content="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/"><meta property="og:site_name" content="Garlicisnotmyfavor"><meta property="og:description" content="ä¸€ç¯‡å…³äºç½‘ç»œåè®®åˆ†ç±»è®ºæ–‡çš„å¤ç°å·¥ä½œ"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://pic3.mac69.com/ico/202010/12105133_55b5814fec.jpeg"><meta property="article:published_time" content="2021-02-05T15:39:55.000Z"><meta property="article:modified_time" content="2021-02-07T14:43:12.000Z"><meta property="article:author" content="Garlic"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://pic3.mac69.com/ico/202010/12105133_55b5814fec.jpeg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/"},"headline":"Garlicisnotmyfavor","image":[],"datePublished":"2021-02-05T15:39:55.000Z","dateModified":"2021-02-07T14:43:12.000Z","author":{"@type":"Person","name":"Garlic"},"description":"ä¸€ç¯‡å…³äºç½‘ç»œåè®®åˆ†ç±»è®ºæ–‡çš„å¤ç°å·¥ä½œ"}</script><link rel="canonical" href="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">garlicisnotmyfavor</a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">é¦–é¡µ</a><a class="navbar-item" href="/archives">å½’æ¡£</a><a class="navbar-item" href="/categories">åˆ†ç±»</a><a class="navbar-item" href="/tags">æ ‡ç­¾</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="ç›®å½•" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="æœç´¢" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="https://pic3.mac69.com/ico/202010/12105133_55b5814fec.jpeg" alt="BSNN"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-02-05T15:39:55.000Z" title="2021-02-05T15:39:55.000Z">2021-02-05</time>å‘è¡¨</span><span class="level-item"><time dateTime="2021-02-07T14:43:12.000Z" title="2021-02-07T14:43:12.000Z">2021-02-07</time>æ›´æ–°</span><span class="level-item"><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87/">è®ºæ–‡</a><span>Â /Â </span><a class="link-muted" href="/categories/%E8%AE%BA%E6%96%87/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%88%86%E7%B1%BB/">ç½‘ç»œåè®®åˆ†ç±»</a></span><span class="level-item">36 åˆ†é’Ÿè¯»å®Œ (å¤§çº¦5425ä¸ªå­—)</span></div></div><h1 class="title is-3 is-size-4-mobile">BSNN</h1><div class="content"><p>ä¸€ç¯‡å…³äºç½‘ç»œåè®®åˆ†ç±»è®ºæ–‡çš„å¤ç°å·¥ä½œ</p>
<a id="more"></a>

<h2 id="è®ºæ–‡ç†è§£"><a href="#è®ºæ–‡ç†è§£" class="headerlink" title="è®ºæ–‡ç†è§£"></a>è®ºæ–‡ç†è§£</h2><p>æœ¬è´¨å…¶å®æ˜¯ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œè§£å†³çš„é—®é¢˜æ˜¯å¯¹ç½‘ç»œåè®®åˆ†ç±»ï¼Œä½†ä¸åŒäºä»¥å¾€çš„åè®®åˆ†ç±»é—®é¢˜ï¼Œè¿™ç¯‡è®ºæ–‡ä¸ä½¿ç”¨æŠ¥æ–‡å¤´ç­‰ç‰¹å¾ä¿¡æ¯ï¼Œè€Œæ˜¯çº¯ç²¹åœ°ä»payloadç‰¹å¾æå–è§’åº¦å¯¹ç½‘ç»œåè®®åˆ†ç±»ã€‚å› æ­¤å¯ä»¥çœ‹ä½œä¸€ä¸ªæ–‡æœ¬åˆ†ç±»é—®é¢˜ï¼Œè¾“å…¥ä¸€ä¸ªpacketçš„payloadéƒ¨åˆ†ï¼Œè¾“å‡ºæŠ¥æ–‡åè®®ç±»å‹ã€‚å…¶ä¼˜ç‚¹åœ¨äºï¼š</p>
<ul>
<li>å¯ä»¥å­¦ä¹ æ–°çš„åè®®ï¼Œè€Œä¸è¢«é™åˆ¶äºå·²æœ‰åè®®</li>
<li>ä¸éœ€è¦ç¹å¤çš„å¯¹æŠ¥æ–‡å¤´ç‰¹å¾ä¿¡æ¯çš„æŒ–æ˜ï¼ˆå¹¶ä¸”è¿™äº›ä¿¡æ¯å¯èƒ½æ˜¯ä¸é è°±çš„ï¼Œå¯ä»¥ä¿®æ”¹çš„è¯ï¼‰</li>
</ul>
<img src="1.png" width = "450" height = "250" align=center />

<p>æ¨¡å‹ç»“æ„å¦‚ä¸Šæ‰€ç¤ºï¼Œæ˜¯ä¸€ä¸ªHierarchical attention networkã€‚</p>
<p>å…ˆè§£é‡Šä¸€ä¸‹æ¨¡å‹ä¸­çš„å„ä¸ªå­—æ®µçš„æ„æ€ã€‚DatagramæŒ‡çš„æ˜¯ä¸€ä¸ªpacketä¸­çš„payloadéƒ¨åˆ†ï¼Œå¦‚ä¸‹å›¾wiresharkæŠ“åŒ…åè“è‰²çš„byteéƒ¨åˆ†ã€‚è¿›ä¸€æ­¥å¯¹Datagramåšåˆ’åˆ†å¯ä»¥å¾—åˆ°å¾ˆå¤šä¸ªé•¿åº¦å‡ç­‰çš„Segmentï¼Œè¿™é‡Œå¦‚æœè®¾å®šæ¯ä¸ªsegmenté•¿åº¦éƒ½ä¸º5çš„è¯ï¼Œä¸€ä¸ªsegmentå°±å¯ä»¥è¡¨è¾¾ä¸ºä¸‹å›¾ä¸­çº¢è‰²æ–¹æ¡†éƒ¨åˆ†ã€‚å¯ä»¥çœ‹åˆ°ä¸€ä¸ªsegment[3b, de, 01, 00, 00]é‡Œæœ‰å¤šä¸ªåå…­è¿›åˆ¶å­—ç¬¦ã€‚</p>
<p>è¿™é‡Œä¸ºäº†æ›´å¥½çš„ç†è§£ï¼Œå¯ä»¥åšä¸€ä¸ªç±»æ¯”ï¼š</p>
<ul>
<li>ä¸€ç¯‡æ–‡ç«  ==&gt;  datagram ==&gt; [3b, de, 01, 00, 00, 01, 00, 00â€¦â€¦]</li>
<li>æ–‡ç« ä¸­çš„å¥å­ ==&gt; segment ==&gt; [3b, de, 01, 00, 00]</li>
<li>å¥å­ä¸­çš„å•è¯ ==&gt; 3b</li>
</ul>
<img src="2.png" width = "450" height = "250" align=center />

<p>ç°åœ¨è¦å¯¹è¿™æ ·ä¸€ä¸ªç»“æ„çš„datagramåšè®­ç»ƒï¼Œæœ€åéœ€è¦å­¦åˆ°å®ƒæ‰€å±çš„åè®®ç±»å‹ã€‚æ¨¡å‹çš„ç›´è§‰å°±æ˜¯æˆ‘ä»¬é¦–å…ˆå»å…³æ³¨ä¸€ä¸ªå¥å­ï¼ˆsegmentï¼‰ä¸­å•è¯ï¼ˆcharacterï¼‰çš„è¡¨è¾¾ï¼Œä½¿ç”¨rnn(LSTM/GRU)è®­ç»ƒå¹¶ä¸”æ¯ä¸ªå•è¯å¯¹äºè¿™ä¸ªå¥å­çš„é‡è¦æ€§æ˜¯ä¸åŒçš„ï¼ˆattentionæœºåˆ¶ï¼‰ï¼Œé€šè¿‡è¿™æ ·ä¸€ä¸ªè¿‡ç¨‹å¾—åˆ°è¿™ä¸ªå¥å­ï¼ˆsegmentï¼‰çš„è¡¨è¾¾ã€‚ä¹‹åå†é‡å¤ç±»ä¼¼çš„è¿‡ç¨‹ï¼Œåœ¨ä¸€ç¯‡æ–‡ç« ï¼ˆdatagramï¼‰ä¸­ï¼Œä¸åŒå¥å­çš„é‡è¦æ€§ä¸åŒï¼Œå…³æ³¨äºâ€œç„¦ç‚¹â€ï¼Œé€šè¿‡ä¸€ä¸ªattention encoderå¾—åˆ°è¿™ç¯‡æ–‡ç« ï¼ˆdatagramï¼‰çš„è¡¨è¾¾ï¼Œä½¿ç”¨softmaxç­‰æœ€åå¾—åˆ°è¿™ä¸ªæ–‡ç« ï¼ˆdatagramï¼‰çš„ç±»åˆ«ã€‚</p>
<p>æ•´ä¸ªæ¨¡å‹æœ‰ä¸¤å±‚attention encoderï¼Œä½¿ç”¨çš„RNN Unitæ˜¯LSTM/GRUï¼ˆæˆ‘åé¢å®ç°çš„GRUï¼Œå› ä¸ºå¯¹æˆ‘çš„ç”µè„‘å‹å¥½ä¸€äº›ğŸ¶ï¼‰ï¼Œä¸”æ˜¯bidirectionalçš„ï¼ˆåŒå‘ï¼‰ï¼Œå› ä¸ºè¿™é‡Œä¸Šä¸‹æ–‡éƒ½æ˜¯æœ‰æ„ä¹‰çš„ï¼Œéœ€è¦å…¨å±€çš„ã€‚å®ç°ç»†èŠ‚ä¸Šä½¿ç”¨Focal Lossï¼Œä¸»è¦æ˜¯ä¸ºäº†å¤„ç†æ ·æœ¬æ•°æ®ä¸å‡è¡¡çš„é—®é¢˜ï¼ˆå¥½å·§ä¸å·§æˆ‘åšæ•°æ®çš„æ—¶å€™æ­£å¥½æ¯ä¸ªç±»åˆ«éƒ½æ˜¯å‡åˆ†çš„ï¼Œä¼¼ä¹å¯¹æˆ‘è¿™ä¸ªæ²¡æœ‰å¤šå¤§ç”¨å¤„ï¼Œæˆ‘çš„é”™ï¼‰ï¼Œè¯„ä¼°æŒ‡æ ‡ä½¿ç”¨$F_1$ã€‚</p>
<h2 id="ä»£ç å¤ç°"><a href="#ä»£ç å¤ç°" class="headerlink" title="ä»£ç å¤ç°"></a>ä»£ç å¤ç°</h2><p>ä»£ç ç›®å½•ä¸­dataæ–‡ä»¶å¤¹å†…ä¸ºæ•°æ®ï¼Œtryæ–‡ä»¶å¤¹ä¸­ä¸ºç¬¬ä¸€æ¬¡å°è¯•çš„pytorchä»£ç ï¼ˆnnç»“æ„æœ‰é—®é¢˜ï¼Œå½“æ—¶ä¸å¤ªææ˜ç™½å¤šå±‚attentionæ€ä¹ˆå†™ï¼Œä½†ä¹Ÿæ˜¯ä¸ªå®è·µè¿‡ç¨‹å…ˆä¿å­˜ä¸‹æ¥ï¼‰ï¼Œrun_BSNNä¿å­˜çš„è¿è¡Œæ•°æ®ï¼Œpre.pyåšæ•°æ®é¢„å¤„ç†ï¼ŒDataUtil.pyåšæ•°æ®paddingåŠåˆ’åˆ†ï¼ŒBSNN.pyæ”¾æ¨¡å‹ï¼Œtrain.pyä¸ºè¿è¡Œå…¥å£ã€‚</p>
<p>tensorflowç‰ˆæœ¬ä¸º1.14ï¼ˆä¸»è¦æƒ³ä½¿ç”¨ä¸€ä¸ªè€ç‰ˆæœ¬çš„åº“ï¼Œé™åˆ°è¿™ä¸ªç‰ˆæœ¬ï¼Œwarningè¿˜æ˜¯æœ‰çš„ğŸ˜…ï¼‰ï¼Œpreä½¿ç”¨çš„pytorchï¼ˆä½†æ˜¯ç›¸å…³æ•°æ®é¢„å¤„ç†å·²è¿è¡Œå‡ºæ¥ä¿å­˜å¥½äº†ï¼Œå› æ­¤ä¸éœ€è¦å†ä½¿ç”¨pytorchè¿è¡Œpre.pyï¼‰</p>
<h3 id="æ•°æ®é¢„å¤„ç†"><a href="#æ•°æ®é¢„å¤„ç†" class="headerlink" title="æ•°æ®é¢„å¤„ç†"></a>æ•°æ®é¢„å¤„ç†</h3><p>è®ºæ–‡ä¸­æ•°æ®é›†æ˜¯è‡ªå·±æ”¶é›†äº†ï¼Œå¹¶ä¸”åœ¨ç½‘ä¸Šæ‰¾äº†ä¸€ä¸‹ç°å­˜æ•°æ®é›†ï¼Œæ¯”è¾ƒéš¾å¾—åˆ°æ¡ä»¶é€‚åˆçš„ï¼Œå› æ­¤è‡ªå·±æµé‡æŠ“åŒ…æ”¶é›†äº†300ç¬”æ•°æ®ï¼ˆå¤šäº†æˆ‘ç”µè„‘ä¼¤ä¸èµ·æ¬¸ğŸ˜­ï¼‰ã€‚æ•°æ®ç‰¹å¾å¦‚ä¸‹ï¼š</p>
<ul>
<li>æŠ“çš„æ˜¯DNSï¼ŒOICQï¼ŒQUICä¸‰ä¸ªåè®®ï¼Œæ¯ä¸ªæŠ“å–äº†100ç¬”ï¼Œå…¶ä¸­åä¸¤ä¸ªæ˜¯googleä½¿ç”¨çš„å¿«ä¼ åè®®å’ŒQQä½¿ç”¨çš„åè®®</li>
<li>ä¸€äº›æŠ¥æ–‡å†…å®¹è¾ƒçŸ­ï¼Œä¸€äº›å¾ˆé•¿ï¼Œè¿™ä¸ªç‰¹ç‚¹ä½¿å¾—åšæ•°æ®paddingæ—¶è¾ƒéš¾é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ç»Ÿä¸€é•¿åº¦ï¼ˆæˆ‘è¿™é‡Œæœ€åé€‰æ‹©çš„100ä½œä¸ºå›ºå®šé•¿ï¼‰</li>
<li>å› ä¸ºæ˜¯è¿ç»­åœ°æŠ“å–ï¼Œå‰åæŠ¥æ–‡ç›¸ä¼¼åº¦å¾ˆå¤šï¼Œå› æ­¤æ¯ç±»åè®®å¤šæ ·æ€§ä¸è¶³</li>
</ul>
<p>é¢„å¤„ç†æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>
<img src="3.png" width = "550" height = "250" align=center />

<p>pre.pyä¸­æ•°æ®å¤„ç†ä»£ç </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸€ä¸ªsegmentçš„é•¿åº¦ï¼Œè®ºæ–‡ä¸­æ¨èçš„æ˜¯L=8</span></span><br><span class="line">SEGMENT_LEN = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">packets = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸‹é¢ä¸‰ä¸ªéƒ½æ˜¯å¯¹æŠ¥æ–‡æ•°æ®åšå¤„ç†ï¼Œæå–å‡ºéœ€è¦çš„payloadéƒ¨åˆ†ï¼Œå­˜å‚¨ä¸‹æ¥</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/QUIC.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    content = json.load(reader)</span><br><span class="line">    <span class="comment">#len(content) å…ˆå¼„å°ä¸€ç‚¹çš„æ•°æ®é›†</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">        packet = [<span class="string">&#x27;0&#x27;</span>, content[i][<span class="string">&quot;_source&quot;</span>][<span class="string">&quot;layers&quot;</span>][<span class="string">&quot;udp&quot;</span>][<span class="string">&quot;udp.payload&quot;</span>]]</span><br><span class="line">        packets.append(packet)</span><br><span class="line">    reader.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># QQçš„åè®®</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/OICQ.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    content = json.load(reader)</span><br><span class="line">    <span class="comment">#len(content) å…ˆå¼„å°ä¸€ç‚¹çš„æ•°æ®é›†</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">        packet = [<span class="string">&#x27;1&#x27;</span>, content[i][<span class="string">&quot;_source&quot;</span>][<span class="string">&quot;layers&quot;</span>][<span class="string">&quot;udp&quot;</span>][<span class="string">&quot;udp.payload&quot;</span>]]</span><br><span class="line">        packets.append(packet)</span><br><span class="line">    reader.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;data/DNS.json&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>) <span class="keyword">as</span> reader:</span><br><span class="line">    content = json.load(reader)</span><br><span class="line">    <span class="comment">#len(content) å…ˆå¼„å°ä¸€ç‚¹çš„æ•°æ®é›†</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">100</span>):</span><br><span class="line">        packet = [<span class="string">&#x27;2&#x27;</span>, content[i][<span class="string">&quot;_source&quot;</span>][<span class="string">&quot;layers&quot;</span>][<span class="string">&quot;udp&quot;</span>][<span class="string">&quot;udp.payload&quot;</span>]]</span><br><span class="line">        packets.append(packet)</span><br><span class="line">    reader.close()</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainæ•°æ®å†™å…¥csv ==&gt; payload.csv</span></span><br><span class="line">csv_fp = <span class="built_in">open</span>(<span class="string">&quot;data/train.csv&quot;</span>, <span class="string">&quot;w&quot;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>, newline=<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">writer = csv.writer(csv_fp)</span><br><span class="line">writer.writerow([<span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;payload&#x27;</span>])</span><br><span class="line">writer.writerows(packets)</span><br><span class="line">csv_fp.close()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">å°†payloadä¸­å­—ç¬¦è½¬åŒ–ä¸º0-255æ•°å­—</span></span><br><span class="line"><span class="string">å°†æ¯ä¸ªdatagramçš„payloadæŒ‰ç…§é•¿åº¦N=SEGMENT_LENåˆ’åˆ†</span></span><br><span class="line"><span class="string">return: [[label, [[0,1,2,3,...][]....]]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment_slice</span>():</span></span><br><span class="line">    data = pd.read_csv(<span class="string">&#x27;data/train.csv&#x27;</span>, encoding=<span class="string">&#x27;UTF-8&#x27;</span>)</span><br><span class="line">    labels = data[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">    payloads = data[<span class="string">&#x27;payload&#x27;</span>].apply(<span class="keyword">lambda</span> x :x.split(<span class="string">&#x27;:&#x27;</span>))</span><br><span class="line">    Datagrams = [] <span class="comment"># [[label, [0,255,...]],...]</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(data)):</span><br><span class="line">        Datagram = [] <span class="comment"># [label, [0,255,...]]</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(payloads[i])):</span><br><span class="line">            Datagram.append(<span class="built_in">int</span>(payloads[i][j], <span class="number">16</span>))</span><br><span class="line">        Datagrams.append([labels[i], Datagram])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆ‡ç‰‡</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(Datagrams)):</span><br><span class="line">        last_num = <span class="built_in">len</span>(Datagrams[i][<span class="number">1</span>]) % SEGMENT_LEN</span><br><span class="line">        Datagrams[i][<span class="number">1</span>] = Datagrams[i][<span class="number">1</span>] + [<span class="number">0</span>]*(SEGMENT_LEN-last_num)</span><br><span class="line">        temp = np.array(Datagrams[i][<span class="number">1</span>])</span><br><span class="line">        temp = temp.reshape(-<span class="number">1</span>, SEGMENT_LEN)</span><br><span class="line">        Datagrams[i][<span class="number">1</span>] = temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Datagrams</span><br></pre></td></tr></table></figure>

<p>DataUtil.pyä¸­è¿›ä¸€æ­¥å¤„ç†</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pre <span class="keyword">as</span> pre</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">datagramçš„segmentæ•°é‡ä¸ä¸€è‡´ï¼Œéƒ½ç»Ÿä¸€ä¸ºmaxlenå¤§å° ç©ºçš„å¡«å……0å‘é‡</span></span><br><span class="line"><span class="string">ä¸è¶³ï¼š0çš„å¡«å……æ¯”è¾ƒå¤š</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_data_x</span>(<span class="params">data_xs, maxlen=<span class="number">100</span>, PAD=<span class="number">0</span></span>):</span>  </span><br><span class="line">    padded_data_xs = []</span><br><span class="line">    <span class="keyword">for</span> data_x <span class="keyword">in</span> data_xs:</span><br><span class="line">        <span class="comment"># ä¸€ä¸ªdatagramçš„</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(data_x) &gt;= maxlen:</span><br><span class="line">            padded_data_x = data_x[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padded_data_x = data_x</span><br><span class="line">            zero_len = maxlen-<span class="built_in">len</span>(padded_data_x)</span><br><span class="line">            zero = np.zeros([zero_len,<span class="number">8</span>], <span class="built_in">int</span>)</span><br><span class="line">            padded_data_x = np.insert(padded_data_x, <span class="built_in">len</span>(data_x), values=zero, axis=<span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">        padded_data_xs.append(padded_data_x)</span><br><span class="line">         </span><br><span class="line">    <span class="keyword">return</span> padded_data_xs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_dataset</span>():</span></span><br><span class="line">    num_classes = <span class="number">3</span></span><br><span class="line">    datagrams = pre.segment_slice()</span><br><span class="line"></span><br><span class="line">    random.shuffle(datagrams) <span class="comment"># æ‰“ä¹±æ•°æ®é›†</span></span><br><span class="line"></span><br><span class="line">    data_x = []</span><br><span class="line">    data_y = []</span><br><span class="line">    <span class="keyword">for</span> datagram <span class="keyword">in</span> datagrams:</span><br><span class="line">        label = datagram[<span class="number">0</span>]</span><br><span class="line">        labels = [<span class="number">0</span>] * num_classes</span><br><span class="line">        labels[label-<span class="number">1</span>] = <span class="number">1</span> <span class="comment"># è½¬åŒ–ä¸ºone-hot</span></span><br><span class="line">        data_y.append(labels)</span><br><span class="line">        data_x.append(datagram[<span class="number">1</span>])</span><br><span class="line">    data_x = pad_data_x(data_x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># åˆ’åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†</span></span><br><span class="line">    length = <span class="built_in">len</span>(data_x)</span><br><span class="line">    train_x, dev_x = data_x[:<span class="built_in">int</span>(length*<span class="number">0.9</span>)], data_x[<span class="built_in">int</span>(length*<span class="number">0.9</span>)+<span class="number">1</span> :]</span><br><span class="line">    train_y, dev_y = data_y[:<span class="built_in">int</span>(length*<span class="number">0.9</span>)], data_y[<span class="built_in">int</span>(length*<span class="number">0.9</span>)+<span class="number">1</span> :]</span><br><span class="line">    <span class="keyword">return</span> train_x, train_y, dev_x, dev_y</span><br></pre></td></tr></table></figure>

<h3 id="æ¨¡å‹æ„å»º"><a href="#æ¨¡å‹æ„å»º" class="headerlink" title="æ¨¡å‹æ„å»º"></a>æ¨¡å‹æ„å»º</h3><p>æ¨¡å‹ä¸»è¦åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼š</p>
<ul>
<li>word encoder ï¼ˆBiGRU layerï¼‰</li>
<li>word attention ï¼ˆAttention layerï¼‰</li>
<li>sentence encoder ï¼ˆBiGRU layerï¼‰</li>
<li>sentence attention ï¼ˆAttention layerï¼‰</li>
</ul>
<h4 id="GRUåŸç†"><a href="#GRUåŸç†" class="headerlink" title="GRUåŸç†"></a>GRUåŸç†</h4><img src="4.png" width = "250" height = "220" align=center />

<p>GRUæ˜¯RNNçš„ä¸€ä¸ªå˜ç§ï¼Œä½¿ç”¨é—¨æœºåˆ¶æ¥è®°å½•å½“å‰åºåˆ—çš„çŠ¶æ€ã€‚åœ¨GRUä¸­æœ‰ä¸¤ç§ç±»å‹çš„é—¨ï¼ˆgateï¼‰: reset gateå’Œupdate gateã€‚è¿™ä¸¤ä¸ªé—¨ä¸€èµ·æ§åˆ¶æ¥å†³å®šå½“å‰çŠ¶æ€æœ‰å¤šå°‘ä¿¡æ¯è¦æ›´æ–°ã€‚</p>
<p>reset gateæ˜¯ç”¨äºå†³å®šå¤šå°‘è¿‡å»çš„ä¿¡æ¯è¢«ç”¨äºç”Ÿæˆå€™é€‰çŠ¶æ€ï¼Œå¦‚æœRtä¸º0ï¼Œè¡¨æ˜å¿˜è®°ä¹‹å‰çš„æ‰€æœ‰çŠ¶æ€ï¼š<br>$$<br>r_t = \sigma(W_rx_r+U_rh_{y-1}+b_r)<br>$$<br>æ ¹æ®reset gateçš„åˆ†æ•°å¯ä»¥è®¡ç®—å‡ºå€™é€‰çŠ¶æ€ï¼š<br>$$<br>\hat{h_t} = tanh(W_hx_t+r_t\oplus(U_hh_{t-1}+b_h))<br>$$<br>update gateæ˜¯ç”¨æ¥å†³å®šç”±å¤šå°‘è¿‡å»çš„ä¿¡æ¯è¢«ä¿ç•™ï¼Œä»¥åŠå¤šå°‘æ–°ä¿¡æ¯è¢«åŠ è¿›æ¥ï¼š<br>$$<br>z_t = \sigma(W_zx_r+U_zh_{t-1}+b_z)<br>$$<br>æœ€åï¼Œéšè—å±‚çŠ¶æ€çš„è®¡ç®—å…¬å¼ï¼Œæœ‰update gateã€å€™é€‰çŠ¶æ€å’Œä¹‹å‰çš„çŠ¶æ€å…±åŒå†³å®šï¼š<br>$$<br>h_t = (1-z_t)\oplus h_{t-1}+z_t \oplus \hat{h_t}<br>$$</p>
<h4 id="AttentionåŸç†"><a href="#AttentionåŸç†" class="headerlink" title="AttentionåŸç†"></a>AttentionåŸç†</h4><img src="5.png" width = "450" height = "200" align=center />

<h4 id="1ã€word-encoder-layer"><a href="#1ã€word-encoder-layer" class="headerlink" title="1ã€word encoder layer"></a>1ã€word encoder layer</h4><p>é¦–å…ˆï¼Œå°†æ¯ä¸ªsegmentä¸­çš„characteråšembeddingè½¬æ¢æˆå‘é‡ï¼ˆè¿™é‡Œä½¿ç”¨one-hotï¼‰ï¼Œç„¶åè¾“å…¥åˆ°åŒå‘GRUç½‘ç»œä¸­ï¼Œç»“åˆä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œè·å¾—è¯¥characterå¯¹åº”çš„éšè—çŠ¶æ€è¾“å‡º$h_{it}=[\overrightarrow{h_{it}}, \overleftarrow{h_{it}}]$<br>$$<br>x_{it} = W_cw_{it}, t \in [i,T]<br>$$</p>
<p>$$<br>\overrightarrow{h_{it}} = \overrightarrow{GRU}(x_{it})<br>$$</p>
<p>$$<br>\overleftarrow{h_{it}} = \overleftarrow{GRU}(x_{it})<br>$$</p>
<h4 id="2ã€word-attention-layer"><a href="#2ã€word-attention-layer" class="headerlink" title="2ã€word attention layer"></a>2ã€word attention layer</h4><p>attentionæœºåˆ¶çš„ç›®çš„å°±æ˜¯è¦æŠŠä¸€ä¸ªsegmentä¸­ï¼Œå¯¹segmentè¡¨è¾¾æœ€é‡è¦çš„characteræ‰¾å‡ºæ¥ï¼Œèµ‹äºˆä¸€ä¸ªæ›´å¤§çš„æ¯”é‡ã€‚</p>
<p>é¦–å…ˆå°†word encoderé‚£ä¸€æ­¥çš„è¾“å‡ºå¾—åˆ°çš„$h_{it}$è¾“å…¥åˆ°ä¸€ä¸ªå•å±‚çš„æ„ŸçŸ¥æœºä¸­å¾—åˆ°ç»“æœ$u_{it}$ä½œä¸ºå…¶éšå«è¡¨ç¤º<br>$$<br>u_{it} = tanh(W_wh_{it}+b_w)<br>$$<br>æ¥ç€ä¸ºäº†è¡¡é‡characterçš„é‡è¦æ€§ï¼Œå®šä¹‰äº†ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„characterå±‚é¢ä¸Šä¸‹æ–‡å‘é‡$u_w$ï¼Œè®¡ç®—å…¶ä¸segmentä¸­æ¯ä¸ªcharaterçš„ç›¸ä¼¼åº¦ï¼Œç„¶åç»è¿‡ä¸€ä¸ª<code>softmax</code>æ“ä½œè·å¾—äº†ä¸€ä¸ªå½’ä¸€åŒ–çš„attentionæƒé‡çŸ©é˜µ$\alpha_{it}$ï¼Œä»£è¡¨segement iä¸­ç¬¬tä¸ªcharaterçš„æƒé‡ï¼š</p>
<p>$$<br>\alpha_{it} = \frac {exp(u_{it}^Tu_w)}{\sum_t(exp(u_{it}^Tu_w))}<br>$$<br>äºæ˜¯ï¼Œsegmentçš„å‘é‡$s_i$å°±å¯ä»¥çœ‹åšæ˜¯segmentä¸­characterçš„å‘é‡çš„åŠ æƒæ±‚å’Œã€‚è¿™é‡Œçš„characterå±‚é¢ä¸Šä¸‹æ–‡å‘é‡æ˜¯$u_w$éšæœºåˆå§‹åŒ–å¹¶ä¸”å¯ä»¥åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­å­¦ä¹ å¾—åˆ°çš„ã€‚<br>$$<br>s_i = \sum_{t} \alpha_{it}h_{it}<br>$$</p>
<h4 id="3ã€sentence-encoder"><a href="#3ã€sentence-encoder" class="headerlink" title="3ã€sentence encoder"></a>3ã€sentence encoder</h4><p>é€šè¿‡ä¸Šè¿°æ­¥éª¤æˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸ªsegmentçš„å‘é‡è¡¨ç¤ºï¼Œç„¶åå¯ä»¥ç”¨ç›¸ä¼¼çš„æ–¹æ³•å¾—åˆ°datagramå‘é‡$h_{i}=[\overrightarrow{h_{i}}, \overleftarrow{h_{i}}]$ï¼š<br>$$<br>\overleftarrow{h_{i}} = \overleftarrow{GRU}(s_{i}), i \in [i,L]<br>$$</p>
<p>$$<br>\overrightarrow{h_{i}} = \overrightarrow{GRU}(s_{i}), i \in [1,L]<br>$$</p>
<h4 id="4ã€sentence-attention"><a href="#4ã€sentence-attention" class="headerlink" title="4ã€sentence attention"></a>4ã€sentence attention</h4><p>å’Œcharacterçº§åˆ«çš„attentionç±»ä¼¼ï¼Œä½¿ç”¨ä¸€ä¸ªsegmentçº§åˆ«çš„ä¸Šä¸‹æ–‡å‘é‡$u_s$,æ¥è¡¡é‡ä¸€ä¸ªsegmentåœ¨datagramçš„é‡è¦æ€§ã€‚<br>$$<br>u_{i} = tanh(W_sh_{i}+b_s)<br>$$</p>
<p>$$<br>\alpha_i = \frac {exp(u_i^Tu_s)}{\sum_t(exp(u_t^Tu_s))}<br>$$</p>
<p>$$<br>d = \sum_{t} \alpha_{i}h_{i}<br>$$</p>
<h4 id="5ã€softmax"><a href="#5ã€softmax" class="headerlink" title="5ã€softmax"></a>5ã€softmax</h4><p>ä¸Šé¢çš„$d$å‘é‡å°±æ˜¯æˆ‘ä»¬çš„åˆ°çš„æœ€åçš„datagramè¡¨ç¤ºï¼Œç„¶åè¾“å…¥ä¸€ä¸ªå…¨è¿æ¥çš„<code>softmax</code>å±‚è¿›è¡Œåˆ†ç±»å°±okäº†ã€‚</p>
<p>BSNN.pyä¸­åŒ…å«äº†æœ€é‡è¦çš„æ¨¡å‹ä»£ç :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> rnn</span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib <span class="keyword">import</span> layers</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSequenceLength</span>(<span class="params">sequences</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param sequences: æ‰€æœ‰çš„segmetné•¿åº¦ï¼Œ[a_size,b_size,c_size,,,]</span></span><br><span class="line"><span class="string">    :return:æ¯ä¸ªsegmentè¿›è¡Œpaddingå‰çš„å®é™…å¤§å°</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    abs_sequences = tf.<span class="built_in">abs</span>(sequences)</span><br><span class="line">    <span class="comment"># after padding data, max is 0</span></span><br><span class="line">    abs_max_seq = tf.reduce_max(abs_sequences, reduction_indices=<span class="number">2</span>)</span><br><span class="line">    max_seq_sign = tf.sign(abs_max_seq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sum is the real length</span></span><br><span class="line">    real_len = tf.reduce_sum(max_seq_sign, reduction_indices=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.cast(real_len, tf.int32)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BSNN</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, max_sentence_num, max_sentence_length, num_classes, vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                 embedding_size, learning_rate, decay_steps, decay_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                 hidden_size, l2_lambda, grad_clip, is_training=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 initializer=tf.random_normal_initializer(<span class="params">stddev=<span class="number">0.1</span></span>)</span>):</span></span><br><span class="line">        </span><br><span class="line">        self.vocab_size = vocab_size <span class="comment"># å¯¹åº”æˆ‘çš„charæ•°é‡ 256</span></span><br><span class="line">        self.max_sentence_num = max_sentence_num </span><br><span class="line">        self.max_sentence_length = max_sentence_length</span><br><span class="line">        self.num_classes = num_classes <span class="comment"># åˆ†ç±»æ•° 3</span></span><br><span class="line">        self.embedding_size = embedding_size <span class="comment"># å¯¹äºcharçš„emb_size=256 è®ºæ–‡ä¸­åˆå§‹åŒ–ä¸ºone-hot</span></span><br><span class="line">        self.hidden_size = hidden_size </span><br><span class="line">        self.learning_rate = learning_rate</span><br><span class="line">        self.decay_rate = decay_rate</span><br><span class="line">        self.decay_steps = decay_steps</span><br><span class="line">        self.l2_lambda = l2_lambda <span class="comment"># l2èŒƒæ•°</span></span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line">        self.initializer = initializer</span><br><span class="line"></span><br><span class="line">        self.global_step = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>, name=<span class="string">&#x27;global_step&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># placeholder</span></span><br><span class="line">        self.input_x = tf.placeholder(tf.int32, [<span class="literal">None</span>, max_sentence_num, max_sentence_length], name=<span class="string">&#x27;input_x&#x27;</span>)</span><br><span class="line">        self.input_y = tf.placeholder(tf.int32, [<span class="literal">None</span>, num_classes], name=<span class="string">&#x27;input_y&#x27;</span>)</span><br><span class="line">        self.dropout_keep_prob = tf.placeholder(tf.float32, name=<span class="string">&#x27;dropout_keep_prob&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        word_embedding = self.word2vec() <span class="comment"># å¯¹äºcharè¡¨è¾¾ï¼Œåˆå§‹åŒ–ä¸ºone-hot</span></span><br><span class="line">        sen_vec = self.sen2vec(word_embedding) <span class="comment"># å¯¹åº”ä¸€ä¸ªsegmentè¡¨è¾¾</span></span><br><span class="line">        doc_vec = self.doc2vec(sen_vec) <span class="comment"># å¯¹åº”datagramè¡¨è¾¾</span></span><br><span class="line"></span><br><span class="line">        self.logits = self.inference(doc_vec)</span><br><span class="line">        self.loss_val = self.loss(self.input_y, self.logits)</span><br><span class="line">        self.train_op = self.train()</span><br><span class="line">        self.prediction = tf.argmax(self.logits, axis=<span class="number">1</span>, name=<span class="string">&#x27;prediction&#x27;</span>)</span><br><span class="line">        self.pred_min = tf.reduce_min(self.prediction)</span><br><span class="line">        self.pred_max = tf.reduce_max(self.prediction)</span><br><span class="line">        self.pred_cnt = tf.bincount(tf.cast(self.prediction, dtype=tf.int32))</span><br><span class="line">        self.label_cnt = tf.bincount(tf.cast(tf.argmax(self.input_y, axis=<span class="number">1</span>), dtype=tf.int32))</span><br><span class="line">        self.accuracy = self.accuracy(self.logits, self.input_y)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">word2vec</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;embedding&#x27;</span>):</span><br><span class="line">            self.embedding_mat = tf.Variable(tf.eye(self.vocab_size), name=<span class="string">&#x27;embedding&#x27;</span>) <span class="comment"># æ„é€ 256ç»´åº¦å•ä½one-hotå‘é‡</span></span><br><span class="line">            <span class="comment"># [batch, sen_in_doc, wrd_in_sent, embedding_size]</span></span><br><span class="line">            word_embedding = tf.nn.embedding_lookup(self.embedding_mat, self.input_x)</span><br><span class="line">            <span class="keyword">return</span> word_embedding</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BidirectionalGRUEncoder</span>(<span class="params">self, inputs, name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        åŒå‘GRUç¼–ç å±‚ï¼Œå°†ä¸€segmentä¸­çš„æ‰€æœ‰characteræˆ–è€…ä¸€ä¸ªdatagramä¸­çš„æ‰€æœ‰segmentè¿›è¡Œç¼–ç å¾—åˆ°ä¸€ä¸ª2xhidden_sizeçš„è¾“å‡ºå‘é‡</span></span><br><span class="line"><span class="string">        ç„¶ååœ¨è¾“å…¥inputsçš„shapeæ˜¯ï¼š</span></span><br><span class="line"><span class="string">        input:[batch, max_time, embedding_size]</span></span><br><span class="line"><span class="string">        output:[batch, max_time, 2*hidden_size]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(name), tf.variable_scope(name, reuse=tf.AUTO_REUSE):  </span><br><span class="line">            fw_gru_cell = rnn.GRUCell(self.hidden_size)</span><br><span class="line">            bw_gru_cell = rnn.GRUCell(self.hidden_size)</span><br><span class="line">            fw_gru_cell = rnn.DropoutWrapper(fw_gru_cell, output_keep_prob=self.dropout_keep_prob)</span><br><span class="line">            bw_gru_cell = rnn.DropoutWrapper(bw_gru_cell, output_keep_prob=self.dropout_keep_prob)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># fw_outputså’Œbw_outputsçš„sizeéƒ½æ˜¯[batch_size, max_time, hidden_size]</span></span><br><span class="line">            (fw_outputs, bw_outputs), (fw_outputs_state, bw_outputs_state) = tf.nn.bidirectional_dynamic_rnn(</span><br><span class="line">                cell_fw=fw_gru_cell, cell_bw=bw_gru_cell, inputs=inputs,</span><br><span class="line">                sequence_length=getSequenceLength(inputs), dtype=tf.float32</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># outputsçš„shapeæ˜¯[batch_size, max_time, hidden_size*2]</span></span><br><span class="line">            outputs = tf.concat((fw_outputs, bw_outputs), <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">AttentionLayer</span>(<span class="params">self, inputs, name</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inputsæ˜¯GRUå±‚çš„è¾“å‡º</span></span><br><span class="line"><span class="string">        inputs: [batch, max_time, 2*hidden_size]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(name):</span><br><span class="line">            context_weight = tf.Variable(tf.truncated_normal([self.hidden_size*<span class="number">2</span>]), name=<span class="string">&#x27;context_weight&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># ä½¿ç”¨å•å±‚MLPå¯¹GRUçš„è¾“å‡ºè¿›è¡Œç¼–ç ï¼Œå¾—åˆ°éšè—å±‚è¡¨ç¤º</span></span><br><span class="line">            <span class="comment"># uit =tanh(Wwhit + bw)</span></span><br><span class="line">            fc = layers.fully_connected(inputs, self.hidden_size*<span class="number">2</span>, activation_fn=tf.nn.tanh)</span><br><span class="line"></span><br><span class="line">            multiply = tf.multiply(fc, context_weight)</span><br><span class="line">            reduce_sum = tf.reduce_sum(multiply, axis=<span class="number">2</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># shape: [batch_size, max_time, 1]</span></span><br><span class="line">            alpha = tf.nn.softmax(reduce_sum, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># shape: [batch_size, hidden_size*2]</span></span><br><span class="line">            atten_output = tf.reduce_sum(tf.multiply(inputs, alpha), axis=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> atten_output</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sen2vec</span>(<span class="params">self, word_embeded</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;sen2vec&#x27;</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            GRUçš„è¾“å…¥tensoræ˜¯[batch_size, max_time,...]ï¼Œåœ¨æ„é€ segmentå‘é‡æ—¶max_timeåº”è¯¥æ˜¯æ¯ä¸ªsegmentçš„é•¿åº¦ï¼Œ</span></span><br><span class="line"><span class="string">            æ‰€ä»¥è¿™é‡Œå°†batch_size*sen_in_docå½“åšæ˜¯batch_sizeï¼Œè¿™æ ·ä¸€æ¥ï¼Œæ¯ä¸ªGRUçš„cellå¤„ç†çš„éƒ½æ˜¯ä¸€ä¸ªcharacterçš„å‘é‡</span></span><br><span class="line"><span class="string">            å¹¶æœ€ç»ˆå°†ä¸€segmentä¸­çš„æ‰€æœ‰characterçš„å‘é‡èåˆï¼ˆAttentionï¼‰åœ¨ä¸€èµ·å½¢æˆsegmentå‘é‡</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            <span class="comment"># shapeï¼š[batch_size * sen_in_doc, word_in_sent, embedding_size]</span></span><br><span class="line">            word_embeded = tf.reshape(word_embeded, [-<span class="number">1</span>, self.max_sentence_length, self.embedding_size])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># shape: [batch_size * sen_in_doc, word_in_sent, hiddeng_size * 2]</span></span><br><span class="line">            word_encoder = self.BidirectionalGRUEncoder(word_embeded, name=<span class="string">&#x27;word_encoder&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># shape: [batch_size * sen_in_doc, hidden_size * 2]</span></span><br><span class="line">            sen_vec = self.AttentionLayer(word_encoder, name=<span class="string">&#x27;word_attention&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> sen_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">doc2vec</span>(<span class="params">self, sen_vec</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;doc2vec&#x27;</span>):</span><br><span class="line">            <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            è·Ÿsen2vecç±»ä¼¼ï¼Œä¸è¿‡è¿™é‡Œæ¯ä¸ªcellå¤„ç†çš„æ˜¯ä¸€ä¸ªsegmentçš„å‘é‡ï¼Œæœ€åèåˆæˆä¸ºdatagramçš„å‘é‡</span></span><br><span class="line"><span class="string">            &quot;&quot;&quot;</span></span><br><span class="line">            sen_vec = tf.reshape(sen_vec, [-<span class="number">1</span>, self.max_sentence_num, self.hidden_size*<span class="number">2</span>])</span><br><span class="line">            <span class="comment"># shape: [batch_sizeï¼Œsen_in_doc, hidden_size * 2]</span></span><br><span class="line">            doc_encoder = self.BidirectionalGRUEncoder(sen_vec, name=<span class="string">&#x27;doc_encoder&#x27;</span>)</span><br><span class="line">            <span class="comment"># shape: [batch_sizeï¼Œhidden_size * 2]</span></span><br><span class="line">            doc_vec = self.AttentionLayer(doc_encoder, name=<span class="string">&#x27;doc_vec&#x27;</span>)</span><br><span class="line">            <span class="keyword">return</span> doc_vec</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inference</span>(<span class="params">self, doc_vec</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;logits&#x27;</span>):</span><br><span class="line">            fc_out = layers.fully_connected(doc_vec, self.num_classes)</span><br><span class="line">            <span class="keyword">return</span> fc_out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">self, logits, input_y</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;accuracy&#x27;</span>):</span><br><span class="line">            predict = tf.argmax(logits, axis=<span class="number">1</span>, name=<span class="string">&#x27;predict&#x27;</span>)</span><br><span class="line">            label = tf.argmax(input_y, axis=<span class="number">1</span>, name=<span class="string">&#x27;label&#x27;</span>)</span><br><span class="line">            acc = tf.reduce_mean(tf.cast(tf.equal(predict, label), tf.float32))</span><br><span class="line">            <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self, input_y, logits</span>):</span></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">            losses = tf.nn.softmax_cross_entropy_with_logits(labels=input_y, logits=logits)</span><br><span class="line">            loss = tf.reduce_mean(losses)</span><br><span class="line">            <span class="keyword">if</span> self.l2_lambda &gt;<span class="number">0</span>:</span><br><span class="line">                l2_loss = tf.add_n([tf.nn.l2_loss(cand_var) <span class="keyword">for</span> cand_var <span class="keyword">in</span> tf.trainable_variables() <span class="keyword">if</span> <span class="string">&#x27;bia&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> cand_var.name])</span><br><span class="line">                loss += self.l2_lambda * l2_loss</span><br><span class="line">            <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        learning_rate = tf.train.exponential_decay(self.learning_rate, self.global_step,</span><br><span class="line">                                                   self.decay_steps, self.decay_rate, staircase=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># use grad_clip to hand exploding or vanishing gradients</span></span><br><span class="line">        optimizer = tf.train.AdamOptimizer(learning_rate)</span><br><span class="line">        grads_and_vars = optimizer.compute_gradients(self.loss_val)</span><br><span class="line">        <span class="keyword">for</span> idx, (grad, var) <span class="keyword">in</span> <span class="built_in">enumerate</span>(grads_and_vars):</span><br><span class="line">            <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                grads_and_vars[idx] = (tf.clip_by_norm(grad, self.grad_clip), var)</span><br><span class="line"></span><br><span class="line">        train_op = optimizer.apply_gradients(grads_and_vars, global_step=self.global_step)</span><br><span class="line">        <span class="keyword">return</span> train_op</span><br></pre></td></tr></table></figure>

<h3 id="è®­ç»ƒå‚æ•°"><a href="#è®­ç»ƒå‚æ•°" class="headerlink" title="è®­ç»ƒå‚æ•°"></a>è®­ç»ƒå‚æ•°</h3><p>è®­ç»ƒå‚æ•°çš„è®¾å®šå¯ä»¥çœ‹ä¸‹é¢ä»£ç ä¸­configurationçš„å†…å®¹ï¼Œè¿™é‡Œè¯´ä¸€ä¸‹è®ºæ–‡ä¸­æåˆ°çš„å‡ ä¸ªå‚æ•°ï¼Œ$\gamma = 0$ï¼Œ$\alpha$å‡ç­‰ï¼Œsegmenté•¿åº¦è®¾å®šä¸º8ï¼Œbatch_sizeä¸º30ï¼ˆä¸åŸæ–‡ä¸€è‡´ï¼‰ï¼Œepoch=10ã€‚</p>
<p>train.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> BSNN <span class="keyword">import</span> BSNN</span><br><span class="line"><span class="keyword">from</span> DataUtil <span class="keyword">import</span> read_dataset</span><br><span class="line"></span><br><span class="line"><span class="comment">#configuration</span></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;learning_rate&quot;</span>,<span class="number">0.001</span>,<span class="string">&quot;learning rate&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;num_epochs&quot;</span>,<span class="number">10</span>,<span class="string">&quot;embedding size&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;batch_size&quot;</span>, <span class="number">30</span>, <span class="string">&quot;Batch size for training/evaluating.&quot;</span>) <span class="comment">#æ‰¹å¤„ç†çš„å¤§å° 32--&gt;128</span></span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;num_classes&quot;</span>, <span class="number">3</span>, <span class="string">&quot;number of classes&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;vocab_size&quot;</span>, <span class="number">256</span>, <span class="string">&quot;vocabulary size&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;decay_steps&quot;</span>, <span class="number">12000</span>, <span class="string">&quot;how many steps before decay learning rate.&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;decay_rate&quot;</span>, <span class="number">0.9</span>, <span class="string">&quot;Rate of decay for learning rate.&quot;</span>) <span class="comment">#0.5ä¸€æ¬¡è¡°å‡å¤šå°‘</span></span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(<span class="string">&quot;ckpt_dir&quot;</span>,<span class="string">&quot;text_han_checkpoint/&quot;</span>,<span class="string">&quot;checkpoint location for the model&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;num_checkpoints&#x27;</span>,<span class="number">5</span>,<span class="string">&#x27;save checkpoints count&#x27;</span>)  </span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;max_sentence_num&#x27;</span>,<span class="number">100</span>,<span class="string">&#x27;max sentence num in a doc&#x27;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;max_sentence_length&#x27;</span>,<span class="number">8</span>,<span class="string">&#x27;max word count in a sentence&#x27;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;embedding_size&quot;</span>,<span class="number">256</span>,<span class="string">&quot;embedding size&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&#x27;hidden_size&#x27;</span>,<span class="number">50</span>,<span class="string">&#x27;cell output size&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_boolean(<span class="string">&quot;is_training&quot;</span>,<span class="literal">True</span>,<span class="string">&quot;is traning.true:tranining,false:testing/inference&quot;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_integer(<span class="string">&quot;validate_every&quot;</span>, <span class="number">100</span>, <span class="string">&quot;Validate every validate_every epochs.&quot;</span>) <span class="comment">#æ¯10è½®åšä¸€æ¬¡éªŒè¯</span></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&#x27;validation_percentage&#x27;</span>,<span class="number">0.1</span>,<span class="string">&#x27;validat data percentage in train data&#x27;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;dropout_keep_prob&quot;</span>, <span class="number">0.5</span>, <span class="string">&quot;Dropout keep probability (default: 0.5)&quot;</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&quot;l2_reg_lambda&quot;</span>, <span class="number">0.0001</span>, <span class="string">&quot;L2 regularization lambda (default: 0.0)&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_float(<span class="string">&#x27;grad_clip&#x27;</span>,<span class="number">2.0</span>,<span class="string">&#x27;grad_clip&#x27;</span>) <span class="comment"># å’Œç±»åˆ«æ•°ç›¸å…³</span></span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_boolean(<span class="string">&quot;allow_soft_placement&quot;</span>, <span class="literal">True</span>, <span class="string">&quot;Allow device soft device placement&quot;</span>)</span><br><span class="line">tf.flags.DEFINE_boolean(<span class="string">&quot;log_device_placement&quot;</span>, <span class="literal">False</span>, <span class="string">&quot;Log placement of ops on devices&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">FLAGS = tf.flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="comment"># load dataset</span></span><br><span class="line">train_x, train_y, dev_x, dev_y = read_dataset()</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;data load finished&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Graph().as_default():</span><br><span class="line">    sess_conf = tf.ConfigProto(</span><br><span class="line">        allow_soft_placement=FLAGS.allow_soft_placement,</span><br><span class="line">        log_device_placement=FLAGS.log_device_placement)</span><br><span class="line">    sess = tf.Session(config=sess_conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> sess.as_default():</span><br><span class="line">        timestamp = <span class="built_in">str</span>(<span class="built_in">int</span>(time.time()))</span><br><span class="line">        out_dir = os.path.abspath(os.path.join(os.path.curdir, <span class="string">&#x27;run_BSNN&#x27;</span>, timestamp))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(out_dir):</span><br><span class="line">            os.makedirs(out_dir)</span><br><span class="line"></span><br><span class="line">        checkpoint_dir = os.path.abspath(os.path.join(out_dir, FLAGS.ckpt_dir))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</span><br><span class="line">            os.makedirs(checkpoint_dir)</span><br><span class="line"></span><br><span class="line">        checkpoint_prefix = os.path.join(checkpoint_dir, <span class="string">&#x27;model&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        bsnn = BSNN(FLAGS.max_sentence_num, FLAGS.max_sentence_length, FLAGS.num_classes, FLAGS.vocab_size, FLAGS.embedding_size,</span><br><span class="line">                  FLAGS.learning_rate, FLAGS.decay_steps, FLAGS.decay_rate, FLAGS.hidden_size, FLAGS.l2_reg_lambda,</span><br><span class="line">                  FLAGS.grad_clip, FLAGS.is_training)</span><br><span class="line">        </span><br><span class="line">        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">train_step</span>(<span class="params">x_batch, y_batch</span>):</span></span><br><span class="line">            feed_dict = &#123;bsnn.input_x: x_batch,</span><br><span class="line">                         bsnn.input_y: y_batch,</span><br><span class="line">                         bsnn.dropout_keep_prob: FLAGS.dropout_keep_prob</span><br><span class="line">                         &#125;</span><br><span class="line">            tmp, step, loss, accuracy, label_cnt, pred_cnt, pred_min, pred_max = sess.run(</span><br><span class="line">                [bsnn.train_op, bsnn.global_step, bsnn.loss_val, bsnn.accuracy, bsnn.label_cnt, bsnn.pred_cnt,</span><br><span class="line">                 bsnn.pred_min, bsnn.pred_max,], feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">&#x27;train_label_cnt: &#x27;</span>, label_cnt)</span><br><span class="line">            print(<span class="string">&#x27;train_min_max:&#x27;</span>, pred_min, pred_max)</span><br><span class="line">            print(<span class="string">&#x27;train_cnt: &#x27;</span>, pred_cnt)</span><br><span class="line">            time_str = datetime.datetime.now().isoformat()</span><br><span class="line">            print(<span class="string">&quot;&#123;&#125;:step &#123;&#125;, loss &#123;:g&#125;, acc &#123;:g&#125;&quot;</span>.<span class="built_in">format</span>(time_str, step, loss, accuracy))</span><br><span class="line">            <span class="keyword">return</span> step</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">dev_step</span>(<span class="params">dev_x, dev_y</span>):</span></span><br><span class="line">            feed_dict = &#123;bsnn.input_x: dev_x,</span><br><span class="line">                         bsnn.input_y: dev_y,</span><br><span class="line">                         bsnn.dropout_keep_prob: <span class="number">1.0</span></span><br><span class="line">                         &#125;</span><br><span class="line">            step, loss, accuracy, label_cnt, pred_cnt = sess.run(</span><br><span class="line">                [bsnn.global_step, bsnn.loss_val, bsnn.accuracy, bsnn.label_cnt, bsnn.pred_cnt,], feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">            time_str = datetime.datetime.now().isoformat()</span><br><span class="line">            print(<span class="string">&quot;dev result: &#123;&#125;:step &#123;&#125;, loss &#123;:g&#125;, acc &#123;:g&#125;&quot;</span>.<span class="built_in">format</span>(time_str, step, loss, accuracy))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.num_epochs):</span><br><span class="line">            print(<span class="string">&#x27;current epoch %s&#x27;</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">270</span>, FLAGS.batch_size):</span><br><span class="line">                x = train_x[i:i + FLAGS.batch_size]</span><br><span class="line">                y = train_y[i:i + FLAGS.batch_size]</span><br><span class="line">                train_step(x, y)</span><br><span class="line">                cur_step = tf.train.global_step(sess, bsnn.global_step)</span><br><span class="line"></span><br><span class="line">            print(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">            dev_step(dev_x, dev_y)</span><br><span class="line">            path = saver.save(sess, checkpoint_prefix, global_step=epoch)</span><br><span class="line">            print(<span class="string">&#x27;Saved model checkpoint to &#123;&#125;\n&#x27;</span>.<span class="built_in">format</span>(path))</span><br></pre></td></tr></table></figure>

<h3 id="ç»“æœ"><a href="#ç»“æœ" class="headerlink" title="ç»“æœ"></a>ç»“æœ</h3><table>
<thead>
<tr>
<th>epoch</th>
<th>ç»“æœ</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>loss 1.17914, acc 0.206897</td>
</tr>
<tr>
<td>2</td>
<td>loss 1.16526, acc 0.206897</td>
</tr>
<tr>
<td>3</td>
<td>loss 1.17013, acc 0.206897</td>
</tr>
<tr>
<td>4</td>
<td>loss 1.16233, acc 0.206897</td>
</tr>
<tr>
<td>5</td>
<td>loss 1.14642, acc 0.448276</td>
</tr>
<tr>
<td>6</td>
<td>loss 1.04893, acc 0.448276</td>
</tr>
<tr>
<td>7</td>
<td>loss 0.898174, acc 0.448276</td>
</tr>
<tr>
<td>8</td>
<td>loss 0.708184, acc 0.827586</td>
</tr>
<tr>
<td>9</td>
<td>loss 0.616511, acc 0.827586</td>
</tr>
<tr>
<td>10</td>
<td>loss 0.4711, acc 0.827586</td>
</tr>
</tbody></table>
<p>éƒ¨åˆ†è¾“å‡ºç»†èŠ‚è§ä¸‹ï¼š</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line">current epoch 1</span><br><span class="line">2021-02-05T23:28:26.090626:step 1, loss 1.18368, acc 0.233333</span><br><span class="line">2021-02-05T23:28:26.623201:step 2, loss 1.17141, acc 0.366667</span><br><span class="line">2021-02-05T23:28:27.112586:step 3, loss 1.17092, acc 0.2</span><br><span class="line">2021-02-05T23:28:27.600048:step 4, loss 1.16999, acc 0.333333</span><br><span class="line">2021-02-05T23:28:28.092669:step 5, loss 1.16888, acc 0.4</span><br><span class="line">2021-02-05T23:28:28.593807:step 6, loss 1.1664, acc 0.4</span><br><span class="line">2021-02-05T23:28:29.085813:step 7, loss 1.17391, acc 0.233333</span><br><span class="line">2021-02-05T23:28:29.577284:step 8, loss 1.16082, acc 0.433333</span><br><span class="line">2021-02-05T23:28:30.078963:step 9, loss 1.17725, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:30.484341:step 9, loss 1.17914, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-0</span><br><span class="line"></span><br><span class="line">current epoch 2</span><br><span class="line">2021-02-05T23:28:31.313493:step 10, loss 1.14458, acc 0.466667</span><br><span class="line">2021-02-05T23:28:31.801910:step 11, loss 1.16036, acc 0.366667</span><br><span class="line">2021-02-05T23:28:32.340277:step 12, loss 1.20258, acc 0.2</span><br><span class="line">2021-02-05T23:28:32.840851:step 13, loss 1.16389, acc 0.333333</span><br><span class="line">2021-02-05T23:28:33.345806:step 14, loss 1.14916, acc 0.4</span><br><span class="line">2021-02-05T23:28:33.865011:step 15, loss 1.15131, acc 0.4</span><br><span class="line">2021-02-05T23:28:34.375607:step 16, loss 1.16807, acc 0.233333</span><br><span class="line">2021-02-05T23:28:34.888499:step 17, loss 1.15018, acc 0.433333</span><br><span class="line">2021-02-05T23:28:35.416810:step 18, loss 1.16337, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:35.609295:step 18, loss 1.16526, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-1</span><br><span class="line"></span><br><span class="line">current epoch 3</span><br><span class="line">2021-02-05T23:28:36.307796:step 19, loss 1.14345, acc 0.466667</span><br><span class="line">2021-02-05T23:28:36.807924:step 20, loss 1.15106, acc 0.366667</span><br><span class="line">2021-02-05T23:28:37.413876:step 21, loss 1.18131, acc 0.2</span><br><span class="line">2021-02-05T23:28:37.979364:step 22, loss 1.1481, acc 0.333333</span><br><span class="line">2021-02-05T23:28:38.528895:step 23, loss 1.13485, acc 0.4</span><br><span class="line">2021-02-05T23:28:39.053765:step 24, loss 1.14369, acc 0.4</span><br><span class="line">2021-02-05T23:28:39.566839:step 25, loss 1.16878, acc 0.233333</span><br><span class="line">2021-02-05T23:28:40.087222:step 26, loss 1.1341, acc 0.433333</span><br><span class="line">2021-02-05T23:28:40.603841:step 27, loss 1.1617, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:40.796326:step 27, loss 1.17013, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-2</span><br><span class="line"></span><br><span class="line">current epoch 4</span><br><span class="line">2021-02-05T23:28:41.602442:step 28, loss 1.1145, acc 0.466667</span><br><span class="line">2021-02-05T23:28:42.144761:step 29, loss 1.14097, acc 0.366667</span><br><span class="line">2021-02-05T23:28:42.693294:step 30, loss 1.20376, acc 0.2</span><br><span class="line">2021-02-05T23:28:43.229859:step 31, loss 1.14151, acc 0.333333</span><br><span class="line">2021-02-05T23:28:43.755452:step 32, loss 1.12037, acc 0.4</span><br><span class="line">2021-02-05T23:28:44.263125:step 33, loss 1.13507, acc 0.4</span><br><span class="line">2021-02-05T23:28:44.771254:step 34, loss 1.16051, acc 0.233333</span><br><span class="line">2021-02-05T23:28:45.294753:step 35, loss 1.11591, acc 0.433333</span><br><span class="line">2021-02-05T23:28:45.826330:step 36, loss 1.14527, acc 0.3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:46.004852:step 36, loss 1.16233, acc 0.206897</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-3</span><br><span class="line"></span><br><span class="line">current epoch 5</span><br><span class="line">2021-02-05T23:28:46.711379:step 37, loss 1.10953, acc 0.466667</span><br><span class="line">2021-02-05T23:28:47.210262:step 38, loss 1.13091, acc 0.366667</span><br><span class="line">2021-02-05T23:28:47.773246:step 39, loss 1.18087, acc 0.2</span><br><span class="line">2021-02-05T23:28:48.286123:step 40, loss 1.13255, acc 0.333333</span><br><span class="line">2021-02-05T23:28:48.789008:step 41, loss 1.11187, acc 0.4</span><br><span class="line">2021-02-05T23:28:49.302116:step 42, loss 1.13908, acc 0.4</span><br><span class="line">2021-02-05T23:28:49.805403:step 43, loss 1.1431, acc 0.266667</span><br><span class="line">2021-02-05T23:28:50.314138:step 44, loss 1.11676, acc 0.433333</span><br><span class="line">2021-02-05T23:28:50.827770:step 45, loss 1.12081, acc 0.366667</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:51.000757:step 45, loss 1.14642, acc 0.448276</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-4</span><br><span class="line"></span><br><span class="line">current epoch 6</span><br><span class="line">2021-02-05T23:28:51.710229:step 46, loss 1.1051, acc 0.566667</span><br><span class="line">2021-02-05T23:28:52.197731:step 47, loss 1.09202, acc 0.5</span><br><span class="line">2021-02-05T23:28:52.747259:step 48, loss 1.15978, acc 0.333333</span><br><span class="line">2021-02-05T23:28:53.266820:step 49, loss 1.09888, acc 0.5</span><br><span class="line">2021-02-05T23:28:53.778483:step 50, loss 1.04828, acc 0.6</span><br><span class="line">2021-02-05T23:28:54.300956:step 51, loss 1.10639, acc 0.5</span><br><span class="line">2021-02-05T23:28:54.807825:step 52, loss 1.06262, acc 0.5</span><br><span class="line">2021-02-05T23:28:55.310532:step 53, loss 1.03075, acc 0.5</span><br><span class="line">2021-02-05T23:28:55.833391:step 54, loss 1.02559, acc 0.466667</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:28:56.026642:step 54, loss 1.04893, acc 0.448276</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-5</span><br><span class="line"></span><br><span class="line">current epoch 7</span><br><span class="line">2021-02-05T23:28:56.747713:step 55, loss 0.996561, acc 0.566667</span><br><span class="line">2021-02-05T23:28:57.233785:step 56, loss 0.973121, acc 0.5</span><br><span class="line">2021-02-05T23:28:57.788470:step 57, loss 1.10553, acc 0.333333</span><br><span class="line">2021-02-05T23:28:58.301127:step 58, loss 0.873215, acc 0.5</span><br><span class="line">2021-02-05T23:28:58.816781:step 59, loss 0.839209, acc 0.633333</span><br><span class="line">2021-02-05T23:28:59.331319:step 60, loss 0.928375, acc 0.533333</span><br><span class="line">2021-02-05T23:28:59.847088:step 61, loss 0.837925, acc 0.5</span><br><span class="line">2021-02-05T23:29:00.344677:step 62, loss 0.871867, acc 0.533333</span><br><span class="line">2021-02-05T23:29:00.813062:step 63, loss 0.89659, acc 0.5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:00.985665:step 63, loss 0.898174, acc 0.448276</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-6</span><br><span class="line"></span><br><span class="line">current epoch 8</span><br><span class="line">2021-02-05T23:29:01.699028:step 64, loss 0.867806, acc 0.633333</span><br><span class="line">2021-02-05T23:29:02.186859:step 65, loss 0.826654, acc 0.633333</span><br><span class="line">2021-02-05T23:29:02.737684:step 66, loss 1.01797, acc 0.433333</span><br><span class="line">2021-02-05T23:29:03.243936:step 67, loss 0.765, acc 0.666667</span><br><span class="line">2021-02-05T23:29:03.746920:step 68, loss 0.721655, acc 0.8</span><br><span class="line">2021-02-05T23:29:04.245246:step 69, loss 0.937242, acc 0.8</span><br><span class="line">2021-02-05T23:29:04.742633:step 70, loss 0.662761, acc 0.766667</span><br><span class="line">2021-02-05T23:29:05.246319:step 71, loss 0.838433, acc 0.8</span><br><span class="line">2021-02-05T23:29:05.758371:step 72, loss 0.69352, acc 0.833333</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:05.941858:step 72, loss 0.708184, acc 0.827586</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-7</span><br><span class="line"></span><br><span class="line">current epoch 9</span><br><span class="line">2021-02-05T23:29:06.681740:step 73, loss 0.615287, acc 0.833333</span><br><span class="line">2021-02-05T23:29:07.155841:step 74, loss 0.634121, acc 0.833333</span><br><span class="line">2021-02-05T23:29:07.706681:step 75, loss 0.863259, acc 0.7</span><br><span class="line">2021-02-05T23:29:08.233564:step 76, loss 0.527865, acc 0.866667</span><br><span class="line">2021-02-05T23:29:08.728556:step 77, loss 0.626868, acc 0.8</span><br><span class="line">2021-02-05T23:29:09.239092:step 78, loss 0.655192, acc 0.833333</span><br><span class="line">2021-02-05T23:29:09.747197:step 79, loss 0.518165, acc 0.833333</span><br><span class="line">2021-02-05T23:29:10.261518:step 80, loss 0.557925, acc 0.866667</span><br><span class="line">2021-02-05T23:29:10.763176:step 81, loss 0.600361, acc 0.833333</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:10.931726:step 81, loss 0.616511, acc 0.827586</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-8</span><br><span class="line"></span><br><span class="line">current epoch 10</span><br><span class="line">2021-02-05T23:29:11.619203:step 82, loss 0.579097, acc 0.833333</span><br><span class="line">2021-02-05T23:29:12.099981:step 83, loss 0.531345, acc 0.833333</span><br><span class="line">2021-02-05T23:29:12.636098:step 84, loss 0.777414, acc 0.7</span><br><span class="line">2021-02-05T23:29:13.135171:step 85, loss 0.438025, acc 0.866667</span><br><span class="line">2021-02-05T23:29:13.644192:step 86, loss 0.477813, acc 0.833333</span><br><span class="line">2021-02-05T23:29:14.140262:step 87, loss 0.545707, acc 0.833333</span><br><span class="line">2021-02-05T23:29:14.631534:step 88, loss 0.440086, acc 0.833333</span><br><span class="line">2021-02-05T23:29:15.129455:step 89, loss 0.427449, acc 0.866667</span><br><span class="line">2021-02-05T23:29:15.646563:step 90, loss 0.474299, acc 0.833333</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dev result: 2021-02-05T23:29:15.826884:step 90, loss 0.4711, acc 0.827586</span><br><span class="line">Saved model checkpoint to C:\Users\Garli\Desktop\BSNN\run_BSNN\1612538901\text_han_checkpoint\model-9</span><br></pre></td></tr></table></figure>

<h3 id="ä¸è¶³"><a href="#ä¸è¶³" class="headerlink" title="ä¸è¶³"></a>ä¸è¶³</h3><ul>
<li>æ•°æ®é›†æ”¶é›†æ—¶é—´æ¯”è¾ƒä»“ä¿ƒï¼Œå› æ­¤æ ·æœ¬ä¸å¤Ÿæ»¡æ„ï¼Œæ•°æ®é‡ç”±äºç”µè„‘æ€§èƒ½é™åˆ¶ä¹Ÿæ¯”è¾ƒå°‘</li>
<li>Focal Lossé’ˆå¯¹æˆ‘çš„æ•°æ®ç‰¹ç‚¹ä½¿ç”¨$\alpha=[[1],[1],[1]],\gamma=0$ç­‰åŒäº<code>softmax_cross_entropy_with_logits</code>ï¼Œæ•ˆæœä¸é”™ï¼Œä½†æ²¡æœ‰åšè¯¦å°½çš„å¯¹æ¯”å®éªŒè¾ƒéš¾çœ‹å‡ºå®ƒçš„ä¼˜åŠ¿ï¼ˆåä¸€ç¯‡æ–‡ç« æˆ‘å·²ç»åšäº†ä¿®æ”¹ï¼‰</li>
<li>embeddingçš„ç»´åº¦è®¾ç½®çš„256ï¼Œä½†æ„Ÿè§‰æ•°æ®è¡¨ç¤ºæ¯”è¾ƒç¨€ç–</li>
<li>å¯¹äºcharacterçš„è¿™ä¸€å±‚çš„è®­ç»ƒï¼Œæˆ‘çš„ç›´è§‰ä¸Šæ¥è¯´æ˜¯è¿œä¸å¦‚å•è¯è®­ç»ƒä¹‹äºå¥å­çš„æ„ä¹‰æ€§ï¼Œå› ä¸ºç›¸åŒçš„byteå¯èƒ½ä¸å…·æœ‰å¾ˆå¼ºçš„å…³è”æ€§ï¼Œå¦‚æœç›´æ¥å°†segmentçš„æ•°å­—è¡¨ç¤ºä½œä¸ºè¾“å…¥ï¼Œå‡å°‘ä¸€å±‚ä¸çŸ¥é“æ•ˆæœæ˜¯å¦ä¼šé€€åŒ–å¾ˆå¤š</li>
</ul>
<h2 id="å‚è€ƒé“¾æ¥"><a href="#å‚è€ƒé“¾æ¥" class="headerlink" title="å‚è€ƒé“¾æ¥"></a>å‚è€ƒé“¾æ¥</h2><p><a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/8624128">https://ieeexplore.ieee.org/abstract/document/8624128</a></p>
<p><a target="_blank" rel="noopener" href="https://www.aclweb.org/anthology/N16-1174.pdf">https://www.aclweb.org/anthology/N16-1174.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://zihuaweng.github.io/2018/04/01/loss/">https://zihuaweng.github.io/2018/04/01/loss/</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/80594704">https://zhuanlan.zhihu.com/p/80594704</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/35571412">https://zhuanlan.zhihu.com/p/35571412</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>BSNN</p><p><a href="http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/">http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>ä½œè€…</h6><p>Garlic</p></div></div><div class="level-item is-narrow"><div><h6>å‘å¸ƒäº</h6><p>2021-02-05</p></div></div><div class="level-item is-narrow"><div><h6>æ›´æ–°äº</h6><p>2021-02-07</p></div></div><div class="level-item is-narrow"><div><h6>è®¸å¯åè®®</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_telegram"></a><a class="a2a_button_whatsapp"></a><a class="a2a_button_reddit"></a></div><script src="https://static.addtoany.com/menu/page.js" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Ÿæ‰“èµä¸€ä¸‹ä½œè€…å§</h3><div class="buttons is-centered"><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>å¾®ä¿¡</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="å¾®ä¿¡"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2021/02/07/FocalLoss/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">FocalLoss</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2021/01/21/iceberg/"><span class="level-item">iceberg</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">è¯„è®º</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'http://garlicisnotmyfavor.xyz/2021/02/05/BSNN/';
            this.page.identifier = '2021/02/05/BSNN/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'garlicisnotmyfavor' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.jpg" alt="Garlic"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Garlic</p><p class="is-size-6 is-block">ä¸€ç”Ÿæ¸©æš–çº¯è‰¯ ä¸èˆçˆ±ä¸è‡ªç”±</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">æ–‡ç« </p><a href="/archives"><p class="title">27</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">åˆ†ç±»</p><a href="/categories"><p class="title">14</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">æ ‡ç­¾</p><a href="/tags"><p class="title">22</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/garlicisnotmyfavor" target="_blank" rel="noopener">å…³æ³¨æˆ‘</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/garlicisnotmyfavor"><i class="fab fa-github"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">ç›®å½•</h3><ul class="menu-list"><li><a class="level is-mobile" href="#è®ºæ–‡ç†è§£"><span class="level-left"><span class="level-item">1</span><span class="level-item">è®ºæ–‡ç†è§£</span></span></a></li><li><a class="level is-mobile" href="#ä»£ç å¤ç°"><span class="level-left"><span class="level-item">2</span><span class="level-item">ä»£ç å¤ç°</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#æ•°æ®é¢„å¤„ç†"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">æ•°æ®é¢„å¤„ç†</span></span></a></li><li><a class="level is-mobile" href="#æ¨¡å‹æ„å»º"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">æ¨¡å‹æ„å»º</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#GRUåŸç†"><span class="level-left"><span class="level-item">2.2.1</span><span class="level-item">GRUåŸç†</span></span></a></li><li><a class="level is-mobile" href="#AttentionåŸç†"><span class="level-left"><span class="level-item">2.2.2</span><span class="level-item">AttentionåŸç†</span></span></a></li><li><a class="level is-mobile" href="#1ã€word-encoder-layer"><span class="level-left"><span class="level-item">2.2.3</span><span class="level-item">1ã€word encoder layer</span></span></a></li><li><a class="level is-mobile" href="#2ã€word-attention-layer"><span class="level-left"><span class="level-item">2.2.4</span><span class="level-item">2ã€word attention layer</span></span></a></li><li><a class="level is-mobile" href="#3ã€sentence-encoder"><span class="level-left"><span class="level-item">2.2.5</span><span class="level-item">3ã€sentence encoder</span></span></a></li><li><a class="level is-mobile" href="#4ã€sentence-attention"><span class="level-left"><span class="level-item">2.2.6</span><span class="level-item">4ã€sentence attention</span></span></a></li><li><a class="level is-mobile" href="#5ã€softmax"><span class="level-left"><span class="level-item">2.2.7</span><span class="level-item">5ã€softmax</span></span></a></li></ul></li><li><a class="level is-mobile" href="#è®­ç»ƒå‚æ•°"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">è®­ç»ƒå‚æ•°</span></span></a></li><li><a class="level is-mobile" href="#ç»“æœ"><span class="level-left"><span class="level-item">2.4</span><span class="level-item">ç»“æœ</span></span></a></li><li><a class="level is-mobile" href="#ä¸è¶³"><span class="level-left"><span class="level-item">2.5</span><span class="level-item">ä¸è¶³</span></span></a></li></ul></li><li><a class="level is-mobile" href="#å‚è€ƒé“¾æ¥"><span class="level-left"><span class="level-item">3</span><span class="level-item">å‚è€ƒé“¾æ¥</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">garlicisnotmyfavor</a><p class="is-size-7"><span>&copy; 2021 Garlic</span>Â Â Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>Â &amp;Â <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="å›åˆ°é¡¶ç«¯" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.cloudflare.com/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "æ­¤ç½‘ç«™ä½¿ç”¨Cookieæ¥æ”¹å–„æ‚¨çš„ä½“éªŒã€‚",
          dismiss: "çŸ¥é“äº†ï¼",
          allow: "å…è®¸ä½¿ç”¨Cookie",
          deny: "æ‹’ç»",
          link: "äº†è§£æ›´å¤š",
          policy: "Cookieæ”¿ç­–",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.cloudflare.com/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="æƒ³è¦æŸ¥æ‰¾ä»€ä¹ˆ..."></div><a class="searchbox-close" href="javascript:;">Ã—</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"æƒ³è¦æŸ¥æ‰¾ä»€ä¹ˆ...","untitled":"(æ— æ ‡é¢˜)","posts":"æ–‡ç« ","pages":"é¡µé¢","categories":"åˆ†ç±»","tags":"æ ‡ç­¾"});
        });</script></body></html>